{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a70616c-f599-4f45-814b-dd1f65563b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading NLP Models...\n",
      "[\n",
      "    {\n",
      "        \"has_pii\": true,\n",
      "        \"detections\": [\n",
      "            {\n",
      "                \"type\": \"PII:FINANCIAL:BIC\",\n",
      "                \"text\": \"GIBACZPX\",\n",
      "                \"start\": 5,\n",
      "                \"end\": 13,\n",
      "                \"confidence\": 1.0,\n",
      "                \"token\": \"[PII:PII_FINANCIAL_BIC_ID_7c6cd69d]\"\n",
      "            }\n",
      "        ],\n",
      "        \"anonymized_text\": \"BIC: [PII:PII_FINANCIAL_BIC_ID_7c6cd69d]\",\n",
      "        \"processing_time_ms\": 24\n",
      "    },\n",
      "    {\n",
      "        \"has_pii\": true,\n",
      "        \"detections\": [\n",
      "            {\n",
      "                \"type\": \"PII:PERSON\",\n",
      "                \"text\": \"Kowalski\",\n",
      "                \"start\": 0,\n",
      "                \"end\": 8,\n",
      "                \"confidence\": 0.9,\n",
      "                \"token\": \"[PII:PII_PERSON_ID_a8393058]\"\n",
      "            }\n",
      "        ],\n",
      "        \"anonymized_text\": \"[PII:PII_PERSON_ID_a8393058] wechselt zur Barmer.\",\n",
      "        \"processing_time_ms\": 6\n",
      "    },\n",
      "    {\n",
      "        \"has_pii\": true,\n",
      "        \"detections\": [\n",
      "            {\n",
      "                \"type\": \"PII:ID:UST\",\n",
      "                \"text\": \"DE287654321\",\n",
      "                \"start\": 10,\n",
      "                \"end\": 21,\n",
      "                \"confidence\": 1.0,\n",
      "                \"token\": \"[PII:PII_ID_UST_ID_ed1aeb0a]\"\n",
      "            }\n",
      "        ],\n",
      "        \"anonymized_text\": \"USt-IdNr. [PII:PII_ID_UST_ID_ed1aeb0a]\",\n",
      "        \"processing_time_ms\": 6\n",
      "    },\n",
      "    {\n",
      "        \"has_pii\": true,\n",
      "        \"detections\": [\n",
      "            {\n",
      "                \"type\": \"PII:ID:DRIVERLICENSE\",\n",
      "                \"text\": \"AA3EFB51059\",\n",
      "                \"start\": 13,\n",
      "                \"end\": 24,\n",
      "                \"confidence\": 1.0,\n",
      "                \"token\": \"[PII:PII_ID_DRIVERLICENSE_ID_385b0137]\"\n",
      "            }\n",
      "        ],\n",
      "        \"anonymized_text\": \"Führerschein [PII:PII_ID_DRIVERLICENSE_ID_385b0137]\",\n",
      "        \"processing_time_ms\": 4\n",
      "    },\n",
      "    {\n",
      "        \"has_pii\": true,\n",
      "        \"detections\": [\n",
      "            {\n",
      "                \"type\": \"PII:DRIVERPLATE\",\n",
      "                \"text\": \"DO-RB 472\",\n",
      "                \"start\": 20,\n",
      "                \"end\": 29,\n",
      "                \"confidence\": 1.0,\n",
      "                \"token\": \"[PII:PII_DRIVERPLATE_ID_37fc2d98]\"\n",
      "            }\n",
      "        ],\n",
      "        \"anonymized_text\": \"Das Kennzeichen ist [PII:PII_DRIVERPLATE_ID_37fc2d98]\",\n",
      "        \"processing_time_ms\": 6\n",
      "    },\n",
      "    {\n",
      "        \"has_pii\": true,\n",
      "        \"detections\": [\n",
      "            {\n",
      "                \"type\": \"PII:FINANCIAL:BIC\",\n",
      "                \"text\": \"NOLADE21KIE\",\n",
      "                \"start\": 46,\n",
      "                \"end\": 57,\n",
      "                \"confidence\": 1.0,\n",
      "                \"token\": \"[PII:PII_FINANCIAL_BIC_ID_d1ba94f9]\"\n",
      "            }\n",
      "        ],\n",
      "        \"anonymized_text\": \"Überweisung an Techniker Krankenkasse, SWIFT: [PII:PII_FINANCIAL_BIC_ID_d1ba94f9]\",\n",
      "        \"processing_time_ms\": 5\n",
      "    },\n",
      "    {\n",
      "        \"has_pii\": true,\n",
      "        \"detections\": [\n",
      "            {\n",
      "                \"type\": \"PII:PIN\",\n",
      "                \"text\": \"847\",\n",
      "                \"start\": 22,\n",
      "                \"end\": 25,\n",
      "                \"confidence\": 1.0,\n",
      "                \"token\": \"[PII:PII_PIN_ID_f4552671]\"\n",
      "            }\n",
      "        ],\n",
      "        \"anonymized_text\": \"gültig bis 03/28, cvv [PII:PII_PIN_ID_f4552671] anzahlung für badezimmer-renovierung: 500€\",\n",
      "        \"processing_time_ms\": 9\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import hashlib\n",
    "import numpy as np\n",
    "import spacy\n",
    "from datetime import datetime\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from spacy.matcher import DependencyMatcher\n",
    "from typing import List, Dict, Any, Set\n",
    "\n",
    "# ==========================================\n",
    "# 1. USER PROVIDED VALIDATORS (STRICT LOGIC)\n",
    "# ==========================================\n",
    "class Validators:\n",
    "    IGNORED_FILE_EXTENSIONS = {\n",
    "        'pdf', 'jpg', 'png', 'gif', 'doc', 'docx', 'xls', 'xlsx',\n",
    "        'txt', 'zip', 'rar', 'exe', 'mp3', 'mp4', 'json', 'xml', 'js', 'py'\n",
    "    }\n",
    "\n",
    "    # Common non-PII words that match ID patterns (Rubbish filter)\n",
    "    # Expanded with false positives from BIC analysis\n",
    "    GERMAN_RUBBISH = {\n",
    "        \"kannst\", \"helfen\", \"beantworten\", \"verstopft\", \"freundlich\", \"formulieren\", \n",
    "        \"morgen\", \"zwischen\", \"kommen\", \"folgene\", \"reingekommen\", \"antwort\", \n",
    "        \"geehrte\", \"herren\", \"mitarbeiter\", \"leider\", \"melden\", \"nettes\", \n",
    "        \"brauche\", \"vorlage\", \"wohnhaft\", \"heisst\", \"nachricht\", \"dringend\",\n",
    "        \"stunde\", \"vertrag\", \"tabelle\", \"eintrag\", \"arbeitge\", \"adresse\",\n",
    "        \"rechnung\", \"folgende\", \"bestimmt\", \"ordentliche\", \"einfache\",\n",
    "        \"lastschrift\", \"gesamtsumme\", \"erfragen\", \"kopieren\", \"kowalski\", \n",
    "        \"belasten\", \"ueberweisen\", \"kreditkarte\", \"erweiterung\", \"einzuziehen\", \n",
    "        \"bergmann\", \"erteilen\", \"wechselt\", \"vorbereiten\", \"lieferanten\", \"geschrieben\"\n",
    "    }\n",
    "\n",
    "    SAFE_TLDS = {\n",
    "        'com', 'net', 'org', 'info', 'biz', 'co', 'io', 'me', 'edu', 'gov', 'int', 'mil',\n",
    "        'de', 'at', 'ch', 'eu', 'nl', 'fr', 'uk', 'be', 'dk', 'no', 'se', 'fi', 'pl', 'it', 'es',\n",
    "        'app', 'dev', 'ai', 'cloud', 'tech', 'digital', 'studio', 'online', 'shop', 'store',\n",
    "        'berlin', 'hamburg', 'koeln', 'bayern'\n",
    "    }\n",
    "\n",
    "    # ISO 3166-1 alpha-2 country codes for BIC validation\n",
    "    ISO_COUNTRY_CODES = {\n",
    "        \"AD\", \"AE\", \"AF\", \"AG\", \"AI\", \"AL\", \"AM\", \"AO\", \"AQ\", \"AR\", \"AS\", \"AT\", \"AU\", \"AW\", \"AX\", \"AZ\",\n",
    "        \"BA\", \"BB\", \"BD\", \"BE\", \"BF\", \"BG\", \"BH\", \"BI\", \"BJ\", \"BL\", \"BM\", \"BN\", \"BO\", \"BQ\", \"BR\", \"BS\",\n",
    "        \"BT\", \"BV\", \"BW\", \"BY\", \"BZ\", \"CA\", \"CC\", \"CD\", \"CF\", \"CG\", \"CH\", \"CI\", \"CK\", \"CL\", \"CM\", \"CN\",\n",
    "        \"CO\", \"CR\", \"CU\", \"CV\", \"CW\", \"CX\", \"CC\", \"CO\", \"KM\", \"CG\", \"CD\", \"CK\", \"CR\", \"CI\", \"HR\", \"CU\", \n",
    "        \"CW\", \"CY\", \"CZ\", \"DE\", \"DJ\", \"DK\", \"DM\", \"DO\", \"DZ\", \"EC\", \"EE\", \"EG\", \"EH\", \"ER\", \"ES\", \"ET\", \n",
    "        \"FI\", \"FJ\", \"FK\", \"FM\", \"FO\", \"FR\", \"GA\", \"GB\", \"GD\", \"GE\", \"GF\", \"GG\", \"GH\", \"GI\", \"GL\", \"GM\", \n",
    "        \"GN\", \"GP\", \"GQ\", \"GR\", \"GS\", \"GT\", \"GU\", \"GW\", \"GY\", \"HK\", \"HM\", \"HN\", \"HR\", \"HT\", \"HU\", \"ID\", \n",
    "        \"IE\", \"IL\", \"IM\", \"IN\", \"IO\", \"IQ\", \"IR\", \"IS\", \"IT\", \"JE\", \"JM\", \"JO\", \"JP\", \"KE\", \"KG\", \"KH\", \n",
    "        \"KI\", \"KM\", \"KN\", \"KP\", \"KR\", \"KW\", \"KY\", \"KZ\", \"LA\", \"LB\", \"LC\", \"LI\", \"LK\", \"LR\", \"LS\", \"LT\", \n",
    "        \"LU\", \"LV\", \"LY\", \"MA\", \"MC\", \"MD\", \"ME\", \"MF\", \"MG\", \"MH\", \"MK\", \"ML\", \"MM\", \"MN\", \"MO\", \"MP\", \n",
    "        \"MQ\", \"MR\", \"MS\", \"MT\", \"MU\", \"MV\", \"MW\", \"MX\", \"MY\", \"MZ\", \"NA\", \"NC\", \"NE\", \"NF\", \"NG\", \"NI\", \n",
    "        \"NL\", \"NO\", \"NP\", \"NR\", \"NU\", \"NZ\", \"OM\", \"PA\", \"PE\", \"PF\", \"PG\", \"PH\", \"PK\", \"PL\", \"PM\", \"PN\", \n",
    "        \"PR\", \"PS\", \"PT\", \"PW\", \"PY\", \"QA\", \"RE\", \"RO\", \"RS\", \"RU\", \"RW\", \"SA\", \"SB\", \"SC\", \"SD\", \"SE\", \n",
    "        \"SG\", \"SH\", \"SI\", \"SJ\", \"SK\", \"SL\", \"SM\", \"SN\", \"SO\", \"SR\", \"SS\", \"ST\", \"SV\", \"SX\", \"SY\", \"SZ\", \n",
    "        \"TC\", \"TD\", \"TF\", \"TG\", \"TH\", \"TJ\", \"TK\", \"TL\", \"TM\", \"TN\", \"TR\", \"TT\", \"TV\", \"TW\", \"TZ\", \"UA\", \n",
    "        \"UG\", \"UM\", \"US\", \"UY\", \"UZ\", \"VA\", \"VC\", \"VE\", \"VG\", \"VI\", \"VN\", \"VU\", \"WF\", \"WS\", \"YE\", \"YT\", \n",
    "        \"ZA\", \"ZM\", \"ZW\"\n",
    "    }\n",
    "\n",
    "    \n",
    "    LOCATION_CONTEXT_WORDS = {\n",
    "        \"anschrift\", \"adresse\", \"wohnt\", \"wohnen\", \"wohnhaft\", \"ging\", \"an\", \"der\", \"die\", \"das\", \n",
    "        \"in\", \"im\", \"er\", \"sie\", \"es\", \"wir\", \"ihr\", \"ist\", \"sind\", \"war\", \"waren\", \"heißt\", \n",
    "        \"heist\", \"objekt\", \"einsatzort\", \"beim\", \"kunden\", \"eltern\", \"seinen\", \"ihren\", \n",
    "        \"meine\", \"unser\", \"unsere\", \"nämlich\", \"namens\", \"heißt\", \"name\", \"herr\", \"frau\",\n",
    "        \"auftraggeber\", \"rechnung\", \"rechnungsadresse\", \"standort\", \"baustelle\",\n",
    "        \"anschrift\", \"adresse\", \"wohnt\", \"wohnen\", \"wohnhaft\", \"ging\", \"an\", \"der\", \"die\", \"das\", \n",
    "        \"in\", \"im\", \"er\", \"sie\", \"es\", \"wir\", \"ihr\", \"ist\", \"sind\", \"war\", \"waren\", \"heißt\", \n",
    "        \"heist\", \"objekt\", \"einsatzort\", \"beim\", \"kunden\", \"eltern\", \"seinen\", \"ihren\", \n",
    "        \"meine\", \"unser\", \"unsere\", \"nämlich\", \"namens\", \"heißt\", \"name\", \"herr\", \"frau\",\n",
    "        \"auftraggeber\", \"rechnung\", \"rechnungsadresse\", \"standort\", \"baustelle\",\n",
    "        \"dr.\", \"prof.\", \"arzt\", \"ärztin\", \"anwalt\", \"familie\"\n",
    "    }\n",
    "\n",
    "    \n",
    "\n",
    "    @staticmethod\n",
    "    def normalize(text: str) -> str:\n",
    "        \"\"\"Removes spaces, dashes, dots, parens for validation.\"\"\"\n",
    "        return re.sub(r\"[\\s\\-\\./\\(\\)\\\\]\", \"\", text)\n",
    "\n",
    "    @staticmethod\n",
    "    def fix_common_typos(text: str) -> str:\n",
    "        \"\"\"Fixes common OCR/Typo errors specifically for financial strings.\"\"\"\n",
    "        return text.upper().replace('O', '0').replace('I', '1').replace('S', '5')\n",
    "\n",
    "    # --- PHONE VALIDATION ---\n",
    "    @staticmethod\n",
    "    def validate_phone(text: str) -> bool:\n",
    "        if re.search(r\"\\d{2}\\.\\d{2}\\.\\-\\d{2}\\.\\d{2}\", text): return False\n",
    "        if re.search(r\"\\d{1,2}[\\.\\/]\\d{1,2}[\\.\\/]\\d{2,4}\", text): return False\n",
    "        \n",
    "        clean = text.lower().replace('o', '0').replace('l', '1')\n",
    "        clean = re.sub(r\"onal|abortel\", \"\", clean)\n",
    "        clean = re.sub(r\"[\\s\\-\\./\\(\\)\\\\]\", \"\", clean)\n",
    "        \n",
    "        if len(clean) < 7 or len(clean) > 15: return False\n",
    "        if len(clean) == 8 and ('.' in text or '/' in text):\n",
    "            if re.search(r\"[01]\\d[\\.\\/][12]\\d{3}\", text) or re.search(r\"[12]\\d{3}[\\.\\/][01]\\d\", text):\n",
    "                return False\n",
    "        if len(set(clean)) <= 2 and len(clean) > 9: return False \n",
    "        if re.search(r\"\\b\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\b\", text): return False \n",
    "        return True\n",
    "\n",
    "    # --- EMAIL VALIDATION ---\n",
    "    @staticmethod\n",
    "    def validate_email(text: str) -> bool:\n",
    "        if \"@\" not in text: return False\n",
    "        parts = text.split('.')\n",
    "        return len(parts[-1]) >= 2 and len(parts[-1]) <= 8\n",
    "\n",
    "    # --- URL VALIDATION ---\n",
    "    @staticmethod\n",
    "    def validate_url(text: str) -> bool:\n",
    "        if \"@\" in text: return False\n",
    "        if text.lower().startswith(('http:', 'https:', 'www.')): return True\n",
    "        domain_part = text.split('/')[0]\n",
    "        parts = domain_part.split('.')\n",
    "        if len(parts) < 2: return False\n",
    "        valid_tld = any(part.lower() in Validators.SAFE_TLDS for part in parts[-2:])\n",
    "        if not valid_tld: return False\n",
    "        last = parts[-1].lower()\n",
    "        return not (len(last) < 2 or last.isdigit() or last in Validators.IGNORED_FILE_EXTENSIONS)\n",
    "\n",
    "    # --- IBAN VALIDATION ---\n",
    "    @staticmethod\n",
    "    def validate_iban(text: str) -> bool:\n",
    "        def check_sum(clean_text):\n",
    "            if \"ZZZ\" in clean_text.upper(): return False\n",
    "            if len(clean_text) < 15 or len(clean_text) > 34: return False\n",
    "            try:\n",
    "                rearranged = clean_text[4:] + clean_text[:4]\n",
    "                numeric_iban = \"\"\n",
    "                for char in rearranged:\n",
    "                    if char.isalpha(): numeric_iban += str(ord(char) - 55)\n",
    "                    elif char.isdigit(): numeric_iban += char\n",
    "                    else: return False\n",
    "                return int(numeric_iban) % 97 == 1\n",
    "            except: return False\n",
    "        clean = Validators.normalize(text).upper()\n",
    "        if check_sum(clean): return True\n",
    "        fixed = Validators.fix_common_typos(clean)\n",
    "        return check_sum(fixed)\n",
    "\n",
    "    # --- CARD VALIDATION ---\n",
    "    @staticmethod\n",
    "    def validate_card(text: str) -> bool:\n",
    "        clean = Validators.normalize(text)\n",
    "        if not clean.isdigit() or not (13 <= len(clean) <= 19) or clean.startswith('0'): \n",
    "            return False\n",
    "        digits = [int(d) for d in clean]\n",
    "        checksum = 0\n",
    "        for i, digit in enumerate(reversed(digits)):\n",
    "            if i % 2 == 1:\n",
    "                doubled = digit * 2\n",
    "                checksum += doubled if doubled < 10 else doubled - 9\n",
    "            else: checksum += digit\n",
    "        return checksum % 10 == 0\n",
    "\n",
    "    @staticmethod\n",
    "    def validate_id(text: str, id_type: str) -> bool:\n",
    "        clean = Validators.normalize(text)\n",
    "        \n",
    "        # --- RUBBISH FILTER ---\n",
    "        if clean.lower() in Validators.GERMAN_RUBBISH: return False\n",
    "        if clean.isalpha() and len(clean) < 12: \n",
    "            if clean.lower() in Validators.GERMAN_RUBBISH: return False\n",
    "\n",
    "        if id_type == \"PII:ID:TAX\":\n",
    "            clean_tax = clean.upper()\n",
    "            if clean_tax.startswith(\"DE\"):\n",
    "                body = clean_tax[2:]\n",
    "                return len(body) == 9 and body.isdigit()\n",
    "            return len(clean_tax) >= 10 and len(clean_tax) <= 13 and clean_tax.isdigit()\n",
    "\n",
    "        if id_type == \"PII:ID:UST\":\n",
    "            clean_ust = clean.upper()\n",
    "            if not clean_ust.startswith(\"DE\"): return False\n",
    "            body = clean_ust[2:]\n",
    "            return len(body) == 9 and body.isdigit()\n",
    "\n",
    "        if id_type == \"PII:ID:INSURANCE\":\n",
    "            return len(clean) == 10 and clean[0].isalpha() and clean[1:].isdigit()\n",
    "\n",
    "        if id_type == \"PII:ID:SVN\": return 9 <= len(clean) <= 15\n",
    "        \n",
    "        if id_type == \"PII:ID:DRIVERLICENSE\":\n",
    "            return 9 <= len(clean) <= 11 and any(c.isdigit() for c in clean) and any(c.isalpha() for c in clean)\n",
    "        \n",
    "        if id_type == \"PII:ID:PASSPORT\":\n",
    "            if clean.isalpha(): return False\n",
    "            return 6 <= len(clean) <= 12\n",
    "            \n",
    "        if id_type == \"PII:ID:NATIONAL\": return 6 <= len(clean) <= 12\n",
    "        \n",
    "        if id_type == \"PII:FINANCIAL:BIC\":\n",
    "            clean_bic = clean.upper()\n",
    "            if len(clean_bic) not in [8, 11]: return False\n",
    "            if clean.lower().endswith(('schrift', 'summe', 'bereit', 'ieren', 'ung')): return False\n",
    "            if not clean_bic[:4].isalpha(): return False\n",
    "            country_code = clean_bic[4:6]\n",
    "            if country_code not in Validators.ISO_COUNTRY_CODES: return False\n",
    "            if text[0].isupper() and text[1:].islower() and text.isalpha(): return False\n",
    "            return True\n",
    "            \n",
    "        return True\n",
    "\n",
    "# ==========================================\n",
    "# 2. CONTEXT VALIDATORS (AI FALLBACK)\n",
    "# ==========================================\n",
    "class IBANContextValidator:\n",
    "    def __init__(self, threshold=0.18):\n",
    "        self.threshold = threshold\n",
    "        self.pos_anchors = [\"IBAN\", \"Konto\", \"Bankverbindung\", \"Überweisung\", \"SEPA\", \"BIC\", \"Bank\", \"Kontodaten\", \"Zahlung an\"]\n",
    "        self.neg_anchors = [\"Telefon\", \"Handy\", \"Fax\", \"ID\", \"Pass\", \"Ausweis\", \"Steuer\", \"SV-Nr\"]\n",
    "        self.vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(2, 4))\n",
    "        self.vectorizer.fit(self.pos_anchors + self.neg_anchors)\n",
    "        self.pos_vectors = self.vectorizer.transform(self.pos_anchors)\n",
    "\n",
    "    def is_valid_context(self, text: str, start: int, end: int) -> bool:\n",
    "        window = text[max(0, start-40):min(len(text), end+40)].lower()\n",
    "        input_vec = self.vectorizer.transform([window])\n",
    "        pos_score = float(np.max(cosine_similarity(input_vec, self.pos_vectors)))\n",
    "        return pos_score > self.threshold\n",
    "\n",
    "class CardContextValidator:\n",
    "    def __init__(self, threshold=0.25):\n",
    "        self.threshold = threshold\n",
    "        self.pos_anchors = [\"Kreditkarte\", \"Mastercard\", \"Visa\", \"Amex\", \"American Express\", \"Karteninhaber\", \"Gültigkeit\", \"endend auf\", \"ending in\", \"Ablaufdatum\", \"Zahlung\", \"Karte\", \"Credit Card\", \"Girocard\", \"EC-Karte\"]\n",
    "        self.neg_anchors = [\"Geburtstag\", \"Telefon\", \"Hausnummer\", \"PLZ\", \"Postleitzahl\", \"Jahr\", \"Uhrzeit\", \"Euro\", \"EUR\", \"PIN\", \"CVV\", \"CVC\", \"Prüfziffer\", \"Code\", \"TAN\", \"IBAN\", \"Bic\", \"Tel\", \"Fax\"]\n",
    "        self.vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(2, 4))\n",
    "        self.vectorizer.fit(self.pos_anchors + self.neg_anchors)\n",
    "        self.pos_vectors = self.vectorizer.transform(self.pos_anchors)\n",
    "        self.neg_vectors = self.vectorizer.transform(self.neg_anchors)\n",
    "\n",
    "    def is_valid_context(self, text: str, start: int, end: int) -> bool:\n",
    "        window = text[max(0, start-60):min(len(text), end+60)].lower()\n",
    "        input_vec = self.vectorizer.transform([window])\n",
    "        pos_score = float(np.max(cosine_similarity(input_vec, self.pos_vectors)))\n",
    "        neg_score = float(np.max(cosine_similarity(input_vec, self.neg_vectors)))\n",
    "        return pos_score > self.threshold and pos_score > neg_score\n",
    "\n",
    "class PhoneContextValidator:\n",
    "    def __init__(self, threshold=0.08):\n",
    "        self.threshold = threshold\n",
    "        self.pos_anchors = [\"tel\", \"telefon\", \"phone\", \"mobil\", \"handy\", \"fon\", \"nummer\", \"nr\", \"rückruf\", \"contact\", \"kontakt\", \"anrufen\", \"angerufen\", \"durchwahl\", \"mobile\", \"erreichbar\", \"unter\", \"ansprechpartner\"]\n",
    "        self.neg_anchors = [\"laufzeit\", \"zeitraum\", \"datum\", \"iban\", \"bic\", \"steuer\", \"id\", \"konto\", \"bank\", \"betrag\", \"euro\", \"eur\", \"plz\", \"gesamtsumme\"]\n",
    "        self.vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(2, 4))\n",
    "        self.vectorizer.fit(self.pos_anchors + self.neg_anchors)\n",
    "        self.pos_vectors = self.vectorizer.transform(self.pos_anchors)\n",
    "        self.neg_vectors = self.vectorizer.transform(self.neg_anchors)\n",
    "\n",
    "    def is_valid_context(self, text: str, start: int, end: int) -> bool:\n",
    "        window = text[max(0, start-60):min(len(text), end+60)].lower()\n",
    "        if any(kw in window for kw in [\"tel\", \"mobil\", \"handy\", \"fon\", \"nummer\", \"nr.\"]): return True\n",
    "        clean_num = re.sub(r\"[^0-9]\", \"\", text[start:end])\n",
    "        if clean_num.startswith(\"01\") and 10 <= len(clean_num) <= 13: return True\n",
    "            \n",
    "        input_vec = self.vectorizer.transform([window])\n",
    "        pos_score = float(np.max(cosine_similarity(input_vec, self.pos_vectors)))\n",
    "        neg_score = float(np.max(cosine_similarity(input_vec, self.neg_vectors)))\n",
    "        return pos_score > self.threshold and (pos_score >= neg_score or pos_score > 0.20)\n",
    "\n",
    "class PassportContextValidator:\n",
    "    def __init__(self, threshold=0.18):\n",
    "        self.threshold = threshold\n",
    "        self.pos_anchors = [\"Pass\", \"Reisepass\", \"Passport\", \"Passnummer\", \"Pass-Nr\", \"Pass No\", \"Visa\", \"Nationalität\", \"Dokument\"]\n",
    "        self.neg_anchors = [\n",
    "            \"IBAN\", \"BIC\", \"Konto\", \"Bank\", \"Euro\", \"Zahlung\", \"Lastschrift\",\n",
    "            \"Telefon\", \"Handy\", \"Tel\", \"Mobil\", \"Nummer\", \"Rückruf\",\n",
    "            \"Jahre\", \"alt\", \"geboren\", \"geb.\", \"Geburtsdatum\",\n",
    "            \"Führerschein\", \"License\", \"FS-Nr\", \"Klasse\", \"Fahrer\",\n",
    "            \"Versicherung\", \"SV-Nr\", \"Krankenkasse\", \"Steuer-ID\"\n",
    "        ]\n",
    "        self.vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(2, 4))\n",
    "        self.vectorizer.fit(self.pos_anchors + self.neg_anchors)\n",
    "        self.pos_vectors = self.vectorizer.transform(self.pos_anchors)\n",
    "        self.neg_vectors = self.vectorizer.transform(self.neg_anchors)\n",
    "\n",
    "    def is_valid_context(self, text: str, start: int, end: int, candidate: str) -> bool:\n",
    "        pre_window = text[max(0, start-25):start].lower()\n",
    "        if \"pass\" in pre_window or \"reise\" in pre_window: return True\n",
    "        window = text[max(0, start-60):min(len(text), end+60)].lower()\n",
    "        input_vec = self.vectorizer.transform([window])\n",
    "        pos_score = float(np.max(cosine_similarity(input_vec, self.pos_vectors)))\n",
    "        neg_score = float(np.max(cosine_similarity(input_vec, self.neg_vectors)))\n",
    "        if candidate.isdigit(): return pos_score > (self.threshold * 1.5) and pos_score > neg_score\n",
    "        return pos_score > self.threshold and pos_score > neg_score\n",
    "\n",
    "class SVNContextValidator:\n",
    "    def __init__(self, threshold=0.15):\n",
    "        self.threshold = threshold\n",
    "        self.pos_anchors = [\"SV-Nummer\", \"SV-Nr\", \"Sozialversicherung\", \"Rentenversicherung\", \"Versicherungsnummer\", \"RV-NR\", \"SVNR\"]\n",
    "        self.neg_anchors = [\"IBAN\", \"Konto\", \"Pass\", \"Telefon\", \"Handy\", \"Steuer\", \"Tax\"]\n",
    "        self.vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(2, 4))\n",
    "        self.vectorizer.fit(self.pos_anchors + self.neg_anchors)\n",
    "        self.pos_vectors = self.vectorizer.transform(self.pos_anchors)\n",
    "\n",
    "    def is_valid_context(self, text: str, start: int, end: int) -> bool:\n",
    "        window = text[max(0, start-50):min(len(text), end+50)].lower()\n",
    "        if any(kw in window for kw in [\"sv-nr\", \"svnr\", \"rv-nr\", \"sozialversicherung\"]): return True\n",
    "        input_vec = self.vectorizer.transform([window])\n",
    "        pos_score = float(np.max(cosine_similarity(input_vec, self.pos_vectors)))\n",
    "        return pos_score > self.threshold\n",
    "\n",
    "class TaxContextValidator:\n",
    "    def __init__(self, threshold=0.20):\n",
    "        self.threshold = threshold\n",
    "        self.pos_anchors = [\"Steuer-ID\", \"Steuernummer\", \"St-ID\", \"USt-IdNr\", \"Finanzamt\", \"Einkommensteuer\", \"Steuererklärung\", \"Steuer-Nr\", \"Identifikationsnummer\", \"Steueridentifikationsnummer\", \"Steuernr\"]\n",
    "        self.neg_anchors = [\"Telefon\", \"Handy\", \"Tel\", \"Mobil\", \"Fax\", \"IBAN\", \"BIC\", \"Konto\", \"Bank\", \"Pass\", \"Ausweis\", \"SV-Nr\", \"Sozialversicherung\", \"Krankenkasse\"]\n",
    "        \n",
    "        self.vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(2, 4))\n",
    "        self.vectorizer.fit(self.pos_anchors + self.neg_anchors)\n",
    "        self.pos_vectors = self.vectorizer.transform(self.pos_anchors)\n",
    "        self.neg_vectors = self.vectorizer.transform(self.neg_anchors)\n",
    "\n",
    "    def is_valid_context(self, text: str, start: int, end: int, candidate: str) -> bool:\n",
    "        pre_window = text[max(0, start-35):start].lower()\n",
    "        if any(kw in pre_window for kw in [\"steuer\", \"id-nr\", \"ust-idnr\", \"st-nr\", \"identifikationsnummer\"]): return True\n",
    "        \n",
    "        window = text[max(0, start-70):min(len(text), end+70)].lower()\n",
    "        input_vec = self.vectorizer.transform([window])\n",
    "        pos_score = float(np.max(cosine_similarity(input_vec, self.pos_vectors)))\n",
    "        neg_score = float(np.max(cosine_similarity(input_vec, self.neg_vectors)))\n",
    "        \n",
    "        if Validators.normalize(candidate).isdigit():\n",
    "            return pos_score > (self.threshold * 1.5) and pos_score > neg_score\n",
    "        return pos_score > self.threshold and pos_score > neg_score\n",
    "\n",
    "class BICContextValidator:\n",
    "    def __init__(self, threshold=0.40):\n",
    "        self.threshold = threshold\n",
    "        self.pos_anchors = [\"BIC\", \"SWIFT\", \"Bank-Code\", \"Bankverbindung\", \"Überweisung\", \"Bankdaten\", \"Kreditinstitut\", \"Zahlungsempfänger\"]\n",
    "        self.neg_anchors = [\"Thorsten\", \"Monika\", \"Detlef\", \"rechnung\", \"folgende\", \"bestimmt\", \"muss\", \"neue\", \"Frau\", \"Herr\", \"Mitarbeiter\", \"Kunde\", \"Name\", \"Telefon\", \"Handy\", \"Adresse\", \"Abwasser\", \"Kaminski\", \"Hartmann\", \"Lastschrift\", \"Gesamtsumme\", \"erfragen\", \"kopieren\", \"Kowalski\", \"belasten\", \"Kreditkarte\", \"Erweiterung\", \"Bergmann\", \"wechselt\"]\n",
    "        \n",
    "        self.vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(2, 4))\n",
    "        self.vectorizer.fit(self.pos_anchors + self.neg_anchors)\n",
    "        self.pos_vectors = self.vectorizer.transform(self.pos_anchors)\n",
    "        self.neg_vectors = self.vectorizer.transform(self.neg_anchors)\n",
    "\n",
    "    def is_valid_context(self, text: str, start: int, end: int, candidate: str) -> bool:\n",
    "        window_lower = text[max(0, start-50):min(len(text), end+50)].lower()\n",
    "        if any(kw in window_lower for kw in [\"bic\", \"swift\", \"bank-code\"]): return True\n",
    "        \n",
    "        input_vec = self.vectorizer.transform([window_lower])\n",
    "        pos_score = float(np.max(cosine_similarity(input_vec, self.pos_vectors)))\n",
    "        neg_score = float(np.max(cosine_similarity(input_vec, self.neg_vectors)))\n",
    "        \n",
    "        if candidate[0].isupper() and candidate[1:].islower():\n",
    "            return pos_score > 0.65 and pos_score > neg_score\n",
    "            \n",
    "        return pos_score > self.threshold and pos_score > neg_score\n",
    "\n",
    "class NationalContextValidator:\n",
    "    def __init__(self, threshold=0.18):\n",
    "        self.threshold = threshold\n",
    "        self.pos_anchors = [\"Ausweis\", \"Personalausweis\", \"National ID\", \"ID-Nr\", \"Identitätskarte\", \"Ausweisnummer\", \"Perso\", \"Dokumentnummer\"]\n",
    "        self.neg_anchors = [\"Führerschein\", \"License\", \"Driver\", \"Klasse\", \"Fahrerlaubnis\", \"Fahrzeug\", \"IBAN\", \"Konto\", \"Telefon\", \"SV-Nr\"]\n",
    "        \n",
    "        self.vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(2, 4))\n",
    "        self.vectorizer.fit(self.pos_anchors + self.neg_anchors)\n",
    "        self.pos_vectors = self.vectorizer.transform(self.pos_anchors)\n",
    "        self.neg_vectors = self.vectorizer.transform(self.neg_anchors)\n",
    "\n",
    "    def is_valid_context(self, text: str, start: int, end: int, candidate: str) -> bool:\n",
    "        pre_window = text[max(0, start-25):start].lower()\n",
    "        if any(kw in pre_window for kw in [\"ausweis\", \"perso\", \"id-nr\"]): return True\n",
    "        \n",
    "        window = text[max(0, start-60):min(len(text), end+60)].lower()\n",
    "        input_vec = self.vectorizer.transform([window])\n",
    "        pos_score = float(np.max(cosine_similarity(input_vec, self.pos_vectors)))\n",
    "        neg_score = float(np.max(cosine_similarity(input_vec, self.neg_vectors)))\n",
    "        \n",
    "        if candidate.isdigit():\n",
    "            return pos_score > (self.threshold * 1.5) and pos_score > neg_score\n",
    "        return pos_score > self.threshold and pos_score > neg_score\n",
    "\n",
    "# ==========================================\n",
    "# ADDED CONTEXT VALIDATORS FROM CODE 2\n",
    "# ==========================================\n",
    "class InsuranceContextValidator(ContextValidatorBase := object): \n",
    "    # Defined within RegexPIIDetector context style of Code 1\n",
    "    def __init__(self, threshold=0.35):\n",
    "        self.threshold = threshold\n",
    "        self.pos_anchors = [\"KV-Nr\", \"Krankenkasse\", \"Versicherung\", \"Mitgliedsnummer\", \"Gesundheitskarte\", \"AOK\", \"Techniker\", \"Barmer\", \"DAK\", \"Versichertennummer\", \"IKK\", \"HUK\", \"Signal Iduna\", \"Allianz\"]\n",
    "        self.neg_anchors = [\"USt-IdNr\", \"Umsatzsteuer\", \"IBAN\", \"Steuernummer\", \"Führerschein\", \"Bankverbindung\", \"BIC\", \"SWIFT\"]\n",
    "        self.vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(2, 4))\n",
    "        self.vectorizer.fit(self.pos_anchors + self.neg_anchors)\n",
    "        self.pos_vectors = self.vectorizer.transform(self.pos_anchors)\n",
    "        self.neg_vectors = self.vectorizer.transform(self.neg_anchors)\n",
    "    def is_valid_context(self, text, start, end):\n",
    "        window = text[max(0, start-60):min(len(text), end+60)].lower()\n",
    "        input_vec = self.vectorizer.transform([window])\n",
    "        pos_score = float(np.max(cosine_similarity(input_vec, self.pos_vectors)))\n",
    "        neg_score = float(np.max(cosine_similarity(input_vec, self.neg_vectors)))\n",
    "        return pos_score > self.threshold and pos_score > neg_score\n",
    "\n",
    "class DriverLicenseContextValidator:\n",
    "    def __init__(self, threshold=0.40):\n",
    "        self.threshold = threshold\n",
    "        self.pos_anchors = [\"Führerschein\", \"Klasse\", \"FS-Nr\", \"Fahrerlaubnis\", \"beantragt\", \"ausgestellt\", \"Fahrerkarte\", \"Pappe\", \"Listen-Nr\", \"Fahrberechtigung\"]\n",
    "        self.neg_anchors = [\"USt-IdNr\", \"DE\", \"Bank\", \"IBAN\", \"Steuer\", \"Umsatzsteuer\", \"Krankenkasse\", \"Versichertennr\", \"Mitgliedsnummer\", \"Reisepass\", \"Pass\", \"Passport\", \"Personalausweis\", \"Ausweis\", \"Identitätskarte\"]\n",
    "        self.vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(2, 4))\n",
    "        self.vectorizer.fit(self.pos_anchors + self.neg_anchors)\n",
    "        self.pos_vectors = self.vectorizer.transform(self.pos_anchors)\n",
    "        self.neg_vectors = self.vectorizer.transform(self.neg_anchors)\n",
    "    def is_valid_context(self, text, start, end):\n",
    "        window = text[max(0, start-60):min(len(text), end+60)].lower()\n",
    "        input_vec = self.vectorizer.transform([window])\n",
    "        pos_score = float(np.max(cosine_similarity(input_vec, self.pos_vectors)))\n",
    "        neg_score = float(np.max(cosine_similarity(input_vec, self.neg_vectors)))\n",
    "        return pos_score > self.threshold and pos_score > neg_score\n",
    "\n",
    "class DriverPlateContextValidator:\n",
    "    def __init__(self, threshold=0.30):\n",
    "        self.threshold = threshold\n",
    "        self.pos_anchors = [\"Kennzeichen\", \"Nummernschild\", \"Auto\", \"PKW\", \"Fahrzeug\", \"geblitzt\", \"Unfall\", \"Halter\", \"Zulassung\", \"abgemeldet\", \"Parkplatz\", \"Falschparker\", \"LKW\"]\n",
    "        self.neg_anchors = [\"Geburtsdatum\", \"IBAN\", \"Telefon\", \"ID-Nr\", \"Versichertennummer\", \"USt-ID\"]\n",
    "        self.vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(2, 4))\n",
    "        self.vectorizer.fit(self.pos_anchors + self.neg_anchors)\n",
    "        self.pos_vectors = self.vectorizer.transform(self.pos_anchors)\n",
    "        self.neg_vectors = self.vectorizer.transform(self.neg_anchors)\n",
    "    def is_valid_context(self, text, start, end):\n",
    "        window = text[max(0, start-60):min(len(text), end+60)].lower()\n",
    "        input_vec = self.vectorizer.transform([window])\n",
    "        pos_score = float(np.max(cosine_similarity(input_vec, self.pos_vectors)))\n",
    "        neg_score = float(np.max(cosine_similarity(input_vec, self.neg_vectors)))\n",
    "        return pos_score > self.threshold and pos_score > neg_score\n",
    "\n",
    "class USTContextValidator:\n",
    "    def __init__(self, threshold=0.40):\n",
    "        self.threshold = threshold\n",
    "        self.pos_anchors = [\"USt-IdNr\", \"USt-ID\", \"Umsatzsteuer\", \"innergemeinschaftlich\", \"Finanzamt\", \"Rechnung\", \"VAT ID\", \"Steuernummer\", \"unternehmerisch\"]\n",
    "        self.neg_anchors = [\"Führerschein\", \"Auto\", \"Fahrer\", \"Unfall\", \"Krankenkasse\", \"Kennzeichen\", \"Fahrzeug\"]\n",
    "        self.vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(2, 4))\n",
    "        self.vectorizer.fit(self.pos_anchors + self.neg_anchors)\n",
    "        self.pos_vectors = self.vectorizer.transform(self.pos_anchors)\n",
    "        self.neg_vectors = self.vectorizer.transform(self.neg_anchors)\n",
    "    def is_valid_context(self, text, start, end):\n",
    "        window = text[max(0, start-60):min(len(text), end+60)].lower()\n",
    "        input_vec = self.vectorizer.transform([window])\n",
    "        pos_score = float(np.max(cosine_similarity(input_vec, self.pos_vectors)))\n",
    "        neg_score = float(np.max(cosine_similarity(input_vec, self.neg_vectors)))\n",
    "        return pos_score > self.threshold and pos_score > neg_score\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class PINContextValidator:\n",
    "    def __init__(self, threshold=0.65):\n",
    "        self.threshold = threshold\n",
    "        # Positive anchors based on your CSV data\n",
    "        self.pos_anchors = [\"CVV\", \"CVC\", \"CVV2\", \"CVC2\", \"Security Code\"]\n",
    "        # Heavy negative anchors to block house numbers and dates\n",
    "        self.neg_anchors = [\"Marktstraße\", \"Straße\", \"PLZ\", \"Hausnummer\", \"Jahr\", \"Datum\", \"Euro\", \"EUR\", \"Summe\", \"Telefon\", \"Tel\", \"Uhr\", \"Minuten\", \"Januar\", \"Februar\"]\n",
    "        self.vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(2, 4))\n",
    "        self.vectorizer.fit(self.pos_anchors + self.neg_anchors)\n",
    "        self.pos_vectors = self.vectorizer.transform(self.pos_anchors)\n",
    "        self.neg_vectors = self.vectorizer.transform(self.neg_anchors)\n",
    "\n",
    "    def is_valid_context(self, text: str, start: int, end: int, candidate: str) -> bool:\n",
    "        # STRICT RULE: Look for \"CVV\", \"CVC\", or \"Prüf\" within 15 characters\n",
    "        proximity_window = text[max(0, start-15):min(len(text), end+15)].lower()\n",
    "        if re.search(r'\\b(cvv|cvc|prüf|security|kartenprüf)\\b', proximity_window):\n",
    "            return True\n",
    "\n",
    "        # AI FALLBACK: For broader sentences, POS score must be high and beat NEG score\n",
    "        window = text[max(0, start-50):min(len(text), end+50)].lower()\n",
    "        input_vec = self.vectorizer.transform([window])\n",
    "        pos_score = float(np.max(cosine_similarity(input_vec, self.pos_vectors)))\n",
    "        neg_score = float(np.max(cosine_similarity(input_vec, self.neg_vectors)))\n",
    "        \n",
    "        return pos_score > self.threshold and pos_score > neg_score\n",
    "\n",
    "# ==========================================\n",
    "# 3. USER PROVIDED DETECTOR\n",
    "# ==========================================\n",
    "class RegexPIIDetector:\n",
    "    def __init__(self, iban_validator=None, card_validator=None, phone_validator=None, passport_validator=None, svn_validator=None, national_validator=None, tax_validator=None, bic_validator=None, insurance_validator=None, license_validator=None, plate_validator=None, ust_validator=None, pin_validator=None):\n",
    "        self.iban_validator = iban_validator\n",
    "        self.card_validator = card_validator\n",
    "        self.phone_validator = phone_validator\n",
    "        self.passport_validator = passport_validator\n",
    "        self.svn_validator = svn_validator\n",
    "        self.national_validator = national_validator\n",
    "        self.tax_validator = tax_validator\n",
    "        self.bic_validator = bic_validator\n",
    "        self.insurance_validator = insurance_validator\n",
    "        self.license_validator = license_validator\n",
    "        self.plate_validator = plate_validator\n",
    "        self.ust_validator = ust_validator\n",
    "        self.pin_validator = pin_validator\n",
    "        \n",
    "        \n",
    "        self.patterns = {\n",
    "            \"PII:FINANCIAL:IBAN\": re.compile(r\"\\b[A-Z]{2}\\d{2}(?:[\\s\\.\\-]*[A-Z0-9]){11,35}\\b\", re.IGNORECASE),\n",
    "            \"PII:FINANCIAL:BIC\": re.compile(r\"\\b[A-Za-z]{4}[A-Za-z]{2}[A-Za-z0-9]{2}(?:[A-Za-z0-9]{3})?\\b\"),\n",
    "            \"PII:FINANCIAL:CARD\": re.compile(r\"\\b[1-9](?:[\\s\\-\\–\\/\\.]*\\d){12,18}\\b\"),\n",
    "            \"PII:FINANCIAL:CARD_PARTIAL_INTERNAL\": re.compile(r\"(?i)(?:visa|mastercard|amex|girocard|kreditkarte|karte|endend|ending)\\s*(?:auf|in|no|nr)?\\s*(?::|#)?\\s*\\b(\\d{4})\\b\"),\n",
    "            #\"PII:CONTACT:URL\": re.compile(r\"\\b(?:(?:https?://|www\\.)[\\w\\-\\.\\/%\\+~=\\?&]+|[\\w\\-]+(?:\\.[a-zA-Z]{2,})+(?:/[\\w\\-\\.\\/%\\+~=\\?&]*)?)\\b\", re.IGNORECASE),\n",
    "            \"PII:CONTACT:PHONE\": re.compile(r\"(?i)(?<![a-zA-Z0-9\\.])(?:(?:(?:\\+|00|[o0])[1-9]\\d{0,4})[\\s\\.\\-\\/\\\\]*(?:\\(\\s*[o0]\\s*\\)\\s*)?|(?:\\(\\s*[o0][1-9]\\d{1,8}\\s*\\)|[o0][1-9]\\d{1,8}))(?:[ \\t\\.\\-\\/\\\\]*(?:onal|abortel)?[ \\t\\.\\-\\/\\\\]*\\d){3,15}[l1]?(?![a-zA-Z0-9])\"),\n",
    "            \"PII:CONTACT:EMAIL\": re.compile(r\"\\b[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,6}(?=[^a-z]|$)\"),\n",
    "            \"PII:ID:UST\": re.compile(r\"\\bDE(?:[\\s\\.\\-]*\\d){9}\\b\", re.IGNORECASE),\n",
    "            \"PII:ID:INSURANCE\": re.compile(r\"\\b[A-Z](?:[\\s\\.\\-]*\\d){9}\\b\", re.IGNORECASE),\n",
    "            \"PII:DRIVERPLATE\": re.compile(r\"\\b[A-ZÄÖÜ]{1,3}-[A-Z]{1,2}\\s?\\d{1,4}[EH]?\\b\", re.IGNORECASE),\n",
    "            \"PII:ID:TAX\": re.compile(r\"\\b(?:\\d{11}|\\d{2}[\\s\\-]\\d{3}[\\s\\-]\\d{3}[\\s\\-]\\d{3}|\\d{2,3}/\\d{3,4}/\\d{4,5}|DE[\\s]?\\d{9})\\b\", re.IGNORECASE),\n",
    "            \"PII:ID:SVN\": re.compile(r\"\\b\\d{2}[\\s]*\\d{6}[\\s]*[A-Z][\\s]*\\d{3}\\b\", re.IGNORECASE),\n",
    "            \"PII:ID:DRIVERLICENSE\": re.compile(r\"\\b[A-Z0-9]{9,11}\\b\", re.IGNORECASE),\n",
    "            \"PII:ID:PASSPORT\": re.compile(r\"\\b[A-Z0-9]{7,11}\\b\", re.IGNORECASE),\n",
    "            \"PII:ID:NATIONAL\": re.compile(r\"\\b[L-Z0-9][0-9A-Z]{8,9}\\b\"),\n",
    "            \"PII:PIN\": re.compile(r\"\\b\\d{3,4}\\b\"),\n",
    "            \"PII:LOCATION:ADDRESS\": re.compile(\n",
    "                r\"\\b(?:[Aa]m|[Zz]um|[Aa]n der|[Ii]n der|[Aa]uf dem|[Bb]ei)?\\s?\"\n",
    "                r\"(?:[A-ZÄÖÜ][a-zäöüßA-Z0-9\\-]*\\s*){1,3}\"\n",
    "                r\"(?:straße|strasse|str\\.|weg|platz|allee|damm|ring|gasse|ufer|chaussee|hof|garten|markt|zeile|wall|graben|kirchweg|landstraße|pfad|autobahn|bundesstraße|zubringer)\"\n",
    "                r\"(?:\\s*(?:[Hh]ausnr\\.?\\s*)?([A-Z]?\\d{1,4}[a-z]?|A\\d+|B\\d+))?\\b\", \n",
    "                re.IGNORECASE\n",
    "            ),            \n",
    "            \"PII:LOCATION:POSTALCODE\": re.compile(r\"\\b(?:PLZ\\s+)?\\d{5}\\b\", re.IGNORECASE),\n",
    "        }\n",
    "\n",
    "    def _validate_match(self, pii_type: str, text: str) -> bool:\n",
    "        if pii_type.startswith(\"PII:ID\") or pii_type == \"PII:FINANCIAL:BIC\" or pii_type == \"PII:DRIVERPLATE\": \n",
    "            return Validators.validate_id(text, pii_type)\n",
    "        #if pii_type == \"PII:CONTACT:URL\": return Validators.validate_url(text)\n",
    "        if pii_type == \"PII:CONTACT:PHONE\": return Validators.validate_phone(text)\n",
    "        elif pii_type == \"PII:CONTACT:EMAIL\": return Validators.validate_email(text)\n",
    "        elif pii_type == \"PII:FINANCIAL:IBAN\": return Validators.validate_iban(text)\n",
    "        elif pii_type == \"PII:FINANCIAL:CARD\": return Validators.validate_card(text)\n",
    "        return True\n",
    "\n",
    "    def _resolve_conflicts(self, detections: List[Dict]) -> List[Dict]:\n",
    "        if not detections: return []\n",
    "        priority = {\n",
    "            \"PII:FINANCIAL:IBAN\": 100, \n",
    "            \"PII:FINANCIAL:CARD\": 95, \n",
    "            \"PII:FINANCIAL:BIC\": 95, \n",
    "            \"PII:ID:UST\": 95, \n",
    "            \"PII:PIN\": 94,\n",
    "            \"PII:LOCATION:ADDRESS\": 92,     \n",
    "            \"PII:ID:SVN\": 90, \n",
    "            \"PII:ID:INSURANCE\": 88,      \n",
    "            \"PII:ID:DRIVERLICENSE\": 87,  \n",
    "            \"PII:DRIVERPLATE\": 86,      \n",
    "            \"PII:ID:PASSPORT\": 85, \n",
    "            \"PII:ID:NATIONAL\": 85, \n",
    "            \"PII:ID:TAX\": 80, \n",
    "            \"PII:CONTACT:EMAIL\": 70,\n",
    "            #\"PII:CONTACT:URL\": 50, \n",
    "            \"PII:CONTACT:PHONE\": 10,\n",
    "            \"PII:LOCATION:POSTALCODE\": 8,\n",
    "        }\n",
    "\n",
    "        # 2. THIS IS THE SPECIFIC LINE:\n",
    "        # It sorts by Priority FIRST, then by the LENGTH of the detected string SECOND.\n",
    "        sorted_dets = sorted(detections, key=lambda x: (-priority.get(x['type'], 0), -(x['end'] - x['start'])))\n",
    "\n",
    "        # 3. Collision logic remains exactly as it was in Code 1\n",
    "        final, occupied = [], set()\n",
    "        for det in sorted_dets:\n",
    "            r = range(det['start'], det['end'])\n",
    "            if not any(i in occupied for i in r):\n",
    "                final.append(det)\n",
    "                occupied.update(r)\n",
    "        \n",
    "        return sorted(final, key=lambda x: x['start'])\n",
    "\n",
    "    def detect(self, text: str) -> List[Dict]:\n",
    "        raw_detections = []\n",
    "        for pii_type, pattern in self.patterns.items():\n",
    "            for match in pattern.finditer(text):\n",
    "                is_valid = False\n",
    "                start_pos, end_pos = match.start(), match.end()\n",
    "                detection_type = pii_type\n",
    "                \n",
    "                if pii_type == \"PII:FINANCIAL:CARD_PARTIAL_INTERNAL\":\n",
    "                    detection_type = \"PII:FINANCIAL:CARD\"\n",
    "                    original_digits = match.group(1)\n",
    "                    clean_text = f\"****{original_digits}\" \n",
    "                    start_pos, end_pos = match.start(1), match.end(1)\n",
    "                    if self.card_validator and self.card_validator.is_valid_context(text, start_pos, end_pos):\n",
    "                        is_valid = True\n",
    "                        \n",
    "                \n",
    "                \n",
    "                elif pii_type == \"PII:FINANCIAL:IBAN\":\n",
    "                    candidate = match.group()\n",
    "                    parts = re.split(r'(\\s+)', candidate)\n",
    "                    stop_labels = {\"BIC\", \"STOP\", \"ICH\", \"NAME\", \"GMBH\", \"IBAN\", \"AN\", \"AUF\", \"AM\", \"BIN\", \"WAR\", \"VON\", \"UND\", \"DIE\", \"IST\", \"DER\", \"DAS\", \"DAME\", \"HERR\", \"WIR\", \"IHR\", \"SEIN\", \"MIT\", \"BEI\"}\n",
    "                    valid_parts = []\n",
    "                    word_idx = 0\n",
    "                    for p in parts:\n",
    "                        if not p.strip(): valid_parts.append(p); continue\n",
    "                        t = p.strip(\".,;:!?() \")\n",
    "                        if not t: continue\n",
    "                        if t.upper() in stop_labels or \"ZZZ\" in t.upper(): break\n",
    "                        if t[0].isupper() and any(c.islower() for c in t): break\n",
    "                        if word_idx > 0 and not any(c.isdigit() for c in t) and len(t) != 4: break\n",
    "                        valid_parts.append(p); word_idx += 1\n",
    "                    clean_text = \"\".join(valid_parts).strip(\".,;:!? \")\n",
    "                    end_pos = match.start() + len(clean_text)\n",
    "                    if Validators.validate_iban(clean_text): is_valid = True\n",
    "                    elif self.iban_validator and self.iban_validator.is_valid_context(text, match.start(), end_pos):\n",
    "                        if 15 <= len(Validators.normalize(clean_text)) <= 34: is_valid = True\n",
    "\n",
    "                elif pii_type == \"PII:FINANCIAL:CARD\":\n",
    "                    if match.start() > 0 and text[match.start()-1] == '+': continue\n",
    "                    clean_text = match.group().strip(\".,;:!? \")\n",
    "                    end_pos = match.start() + len(clean_text)\n",
    "                    if Validators.validate_card(clean_text): is_valid = True\n",
    "                    elif self.card_validator and self.card_validator.is_valid_context(text, match.start(), end_pos): is_valid = True\n",
    "                \n",
    "                elif pii_type == \"PII:ID:DRIVERLICENSE\":\n",
    "                    clean_text = match.group().strip(\".,;:!? \\n\\r\\t\")\n",
    "                    end_pos = match.start() + len(clean_text)\n",
    "                    if Validators.validate_id(clean_text, pii_type) and not clean_text.upper().startswith(\"DE\"):\n",
    "                        if self.license_validator and self.license_validator.is_valid_context(text, match.start(), end_pos):\n",
    "                            is_valid = True\n",
    "\n",
    "                elif pii_type == \"PII:ID:INSURANCE\":\n",
    "                    clean_text = match.group().strip(\".,;:!? \\n\\r\\t\")\n",
    "                    end_pos = match.start() + len(clean_text)\n",
    "                    if Validators.validate_id(clean_text, pii_type):\n",
    "                        if self.insurance_validator and self.insurance_validator.is_valid_context(text, match.start(), end_pos):\n",
    "                            is_valid = True\n",
    "\n",
    "                elif pii_type == \"PII:ID:UST\":\n",
    "                    clean_text = match.group().strip(\".,;:!? \\n\\r\\t\")\n",
    "                    end_pos = match.start() + len(clean_text)\n",
    "                    if Validators.validate_id(clean_text, pii_type):\n",
    "                        if self.ust_validator and self.ust_validator.is_valid_context(text, match.start(), end_pos):\n",
    "                            is_valid = True\n",
    "\n",
    "                elif pii_type == \"PII:DRIVERPLATE\":\n",
    "                    clean_text = match.group().strip(\".,;:!? \\n\\r\\t\")\n",
    "                    end_pos = match.start() + len(clean_text)\n",
    "                    if self.plate_validator and self.plate_validator.is_valid_context(text, match.start(), end_pos):\n",
    "                        is_valid = True\n",
    "\n",
    "                elif pii_type == \"PII:CONTACT:PHONE\":\n",
    "                    clean_text = match.group().strip(\".,;:!? \\n\\r\\t\")\n",
    "                    end_pos = match.start() + len(clean_text)\n",
    "                    if Validators.validate_phone(clean_text):\n",
    "                        if self.phone_validator:\n",
    "                            if self.phone_validator.is_valid_context(text, match.start(), end_pos):\n",
    "                                is_valid = True\n",
    "                        else: is_valid = True\n",
    "\n",
    "                elif pii_type == \"PII:ID:PASSPORT\":\n",
    "                    clean_text = match.group().strip(\".,;:!? \\n\\r\\t\")\n",
    "                    end_pos = match.start() + len(clean_text)\n",
    "                    if Validators.validate_id(clean_text, pii_type):\n",
    "                        if self.passport_validator and self.passport_validator.is_valid_context(text, start_pos, end_pos, clean_text):\n",
    "                            is_valid = True\n",
    "\n",
    "                elif pii_type == \"PII:ID:SVN\":\n",
    "                    clean_text = match.group().strip(\".,;:!? \\n\\r\\t\")\n",
    "                    end_pos = match.start() + len(clean_text)\n",
    "                    if Validators.validate_id(clean_text, pii_type):\n",
    "                        if self.svn_validator:\n",
    "                            if self.svn_validator.is_valid_context(text, match.start(), end_pos):\n",
    "                                is_valid = True\n",
    "                        else: is_valid = True\n",
    "\n",
    "                elif pii_type == \"PII:ID:NATIONAL\":\n",
    "                    clean_text = match.group().strip(\".,;:!? \\n\\r\\t\")\n",
    "                    end_pos = match.start() + len(clean_text)\n",
    "                    if Validators.validate_id(clean_text, pii_type):\n",
    "                        if self.national_validator:\n",
    "                            if self.national_validator.is_valid_context(text, match.start(), end_pos, clean_text):\n",
    "                                is_valid = True\n",
    "                        else: is_valid = True\n",
    "\n",
    "                elif pii_type == \"PII:ID:TAX\":\n",
    "                    clean_text = match.group().strip(\".,;:!? \\n\\r\\t\")\n",
    "                    end_pos = match.start() + len(clean_text)\n",
    "                    if Validators.validate_id(clean_text, pii_type):\n",
    "                        if self.tax_validator:\n",
    "                            if self.tax_validator.is_valid_context(text, start_pos, end_pos, clean_text):\n",
    "                                is_valid = True\n",
    "                        else: is_valid = True\n",
    "                \n",
    "                elif pii_type == \"PII:FINANCIAL:BIC\":\n",
    "                    clean_text = match.group().strip(\".,;:!? \\n\\r\\t\")\n",
    "                    end_pos = match.start() + len(clean_text)\n",
    "                    if Validators.validate_id(clean_text, pii_type):\n",
    "                        if self.bic_validator:\n",
    "                            if self.bic_validator.is_valid_context(text, start_pos, end_pos, clean_text):\n",
    "                                is_valid = True\n",
    "                        else: is_valid = True\n",
    "\n",
    "                elif pii_type == \"PII:PIN\":\n",
    "                    clean_text = match.group().strip(\".,:; \")\n",
    "                    if self.pin_validator and self.pin_validator.is_valid_context(text, match.start(), match.end(), clean_text):\n",
    "                        is_valid = True\n",
    "\n",
    "                else:\n",
    "                    clean_text = match.group().strip(\".,;:!? \")\n",
    "                    end_pos = match.start() + len(clean_text)\n",
    "                    is_valid = self._validate_match(pii_type, clean_text)\n",
    "\n",
    "                if is_valid:\n",
    "                    raw_detections.append({\"type\": detection_type, \"text\": clean_text, \"start\": start_pos, \"end\": end_pos, \"confidence\": 1.0})\n",
    "        return self._resolve_conflicts(raw_detections)\n",
    "\n",
    "# ==========================================\n",
    "# 4. AGE EXTRACTION (CONTEXTUAL)\n",
    "# ==========================================\n",
    "# ==========================================\n",
    "# 4. AGE & BIRTHDAY EXTRACTION (OPTIMIZED)\n",
    "# ==========================================\n",
    "class FastAgeExtractor:\n",
    "    def __init__(self, threshold=0.65):\n",
    "        self.threshold = threshold\n",
    "        self.current_year = datetime.now().year\n",
    "        \n",
    "        # Positive anchors for Age brackets\n",
    "        self.age_pos_anchors = [\n",
    "            \"Ich bin <NUM> Jahre alt\", \"Er ist <NUM> geworden\", \"Sie ist <NUM>\", \n",
    "            \"Mein Alter ist <NUM>\", \"Das Kind ist <NUM>\", \"Ein <NUM>-Jähriger\", \n",
    "            \"Mit <NUM> Jahren\", \"Alter: <NUM>\", \"Jahre alt\", \"(<NUM> Jahre)\",\n",
    "            \"ist <NUM>\", \"schon <NUM>\", \"bin <NUM>\", \"bin erst <NUM>\", \"gerade <NUM>\",\n",
    "            \"im Alter von <NUM>\", \"Sohn, <NUM> Jahre\", \"Tochter, <NUM> Jahre\",\n",
    "            \"Frau Sabine (<NUM>)\", \"H. Goldberg (<NUM> Jahre)\",\"bin <NUM> Jahre alt\", \"ist <NUM> geworden\", \"Alter: <NUM>\", \"Dame ist <NUM>\",\n",
    "            \"bin erst <NUM>\", \"Tochter (<NUM>)\", \"Sohn, <NUM> Jahre\", \"bin <NUM> und kenn mich\",\n",
    "            \"H. Goldberg (<NUM> Jahre)\", \"ca <NUM>, sehr schwerhörig\", \"bin ich <NUM> und Frühaufsteher\",\n",
    "            \"war glaub so <NUM> oder so alt\", \"klang so <NUM>-<NUM> Jahre alt\",\n",
    "            \n",
    "        ]\n",
    "        \n",
    "        # Positive anchors for Birthdays (DOB)\n",
    "        self.dob_pos_anchors = [\n",
    "            \"Geboren am <NUM>\", \"Mein Geburtsdatum ist <NUM>\", \"geb. <NUM>\", \n",
    "            \"geb: <NUM>\", \"Geburtsdatum: <NUM>\", \"Dob: <NUM>\", \"Date of Birth: <NUM>\",\n",
    "            \"geboren am\", \"geboren am <NUM> in\", \"geb <NUM>\", \"geb: <NUM>\",\n",
    "            \"Geburtstag am <NUM>\", \"Jahrgang <NUM>\", \"Baujahr <NUM>\", \"geb. am <NUM>\"\n",
    "        ]\n",
    "        \n",
    "        # Aggressive negative anchors to block context noise (times, prices, IDs, etc.)\n",
    "        self.neg_anchors = [\n",
    "            \"Das kostet <NUM> Euro\", \"Preis <NUM> EUR\", \"Summe <NUM>\", \"Betrag <NUM>\",\n",
    "            \"Hausnummer <NUM>\", \"Hausnr <NUM>\", \"PLZ <NUM>\", \"Postleitzahl <NUM>\",\n",
    "            \"um <NUM> Uhr\", \"gegen <NUM> Uhr\", \"ab <NUM> Uhr\", \"bis <NUM> Uhr\",\n",
    "            \"in <NUM> Minuten\", \"für <NUM> tage\", \"seit <NUM> tagen\", \"vor <NUM> jahren\",\n",
    "            \"<NUM> prozent\", \"<NUM> kg\", \"<NUM> m\", \"artikel <NUM>\", \"nummer <NUM>\",\n",
    "            \"am <NUM>.\", \"vom <NUM>.\", \"den <NUM>.\", \"bis zum <NUM>.\", \"am <NUM> <NUM>\",\n",
    "            \"Klasse <NUM>\", \"Nr. <NUM>\", \"Zimmer <NUM>\", \"Gebäude <NUM>\", \"Stock <NUM>\", \"OG <NUM>\",\n",
    "            \"rechnungsdatum <NUM>\", \"bestelldatum <NUM>\", \"fängt am <NUM>\", \"termin am <NUM>\",\n",
    "            \"am <NUM> waren Ihre Mitarbeiter\", \"Auftrag vom <NUM>\", \"war am <NUM> bei Familie\",\n",
    "            \"Rechnung vom <NUM>\", \"fängt am <NUM> an\", \"termin am <NUM>\", \"bis zum <NUM>\",\n",
    "            \"rechnungsnummer 2024-<NUM>\", \"offener betrag <NUM>\", \"den <NUM>.\", \"vom <NUM>.\",\n",
    "            # Counts & Codes\n",
    "            \"Hausnummer <NUM>\", \"Hausnr <NUM>\", \"PLZ <NUM>\", \"Postleitzahl <NUM>\",\n",
    "            \"Betrag <NUM> Euro\", \"Preis <NUM> EUR\", \"Summe <NUM>\", \"<NUM> Parteien\",\n",
    "            \"Rabatt von <NUM> %\", \"seit <NUM> Tagen\", \"vor <NUM> Jahren\",\n",
    "            # Communication Noise\n",
    "            \"um <NUM> Uhr\", \"gegen <NUM> Uhr\", \"ab <NUM> Uhr\", \"bis <NUM> Uhr\",\n",
    "            \"Ticket #<NUM>\", \"Durchwahl -<NUM>\", \"unter <NUM> erreichbar\", \"Zimmer <NUM>\"\n",
    "\n",
    "        ]\n",
    "        \n",
    "        # Use ngram_range (1,3) to capture multi-word context like \"geboren am\"\n",
    "        self.vectorizer = TfidfVectorizer(analyzer='word', ngram_range=(1, 3))\n",
    "        self.vectorizer.fit(self.age_pos_anchors + self.dob_pos_anchors + self.neg_anchors)\n",
    "        \n",
    "        self.age_pos_vectors = self.vectorizer.transform(self.age_pos_anchors)\n",
    "        self.dob_pos_vectors = self.vectorizer.transform(self.dob_pos_anchors)\n",
    "        self.neg_vectors = self.vectorizer.transform(self.neg_anchors)\n",
    "\n",
    "    def calculate_age(self, value_str):\n",
    "        # returns (age_value, is_birthday_format)\n",
    "        if re.search(r'\\d{1,2}[./-]\\d{1,2}[./-]\\d{4}', value_str):\n",
    "            for fmt in (\"%d.%m.%Y\", \"%d/%m/%Y\", \"%d-%m-%Y\"):\n",
    "                try:\n",
    "                    dob = datetime.strptime(value_str, fmt)\n",
    "                    age = self.current_year - dob.year - ((datetime.now().month, datetime.now().day) < (dob.month, dob.day))\n",
    "                    return age, True\n",
    "                except ValueError: continue\n",
    "        try:\n",
    "            val = int(value_str)\n",
    "            # If 4 digits, assume it's a birth year contextually\n",
    "            if 1900 < val <= self.current_year:\n",
    "                return self.current_year - val, True\n",
    "            return val, False\n",
    "        except ValueError: return None, False\n",
    "\n",
    "    def get_pii_type(self, age, is_birthday):\n",
    "        if age is None or age < 0 or age > 110:\n",
    "            return None\n",
    "        if is_birthday: return \"PII:BIRTHDAY\"\n",
    "        if age < 12: return \"PII:AGE:CHILD\"\n",
    "        elif 12 <= age <= 17: return \"PII:AGE:TEEN\"\n",
    "        elif 18 <= age <= 64: return \"PII:AGE:ADULT\"\n",
    "        else: return \"PII:AGE:SENIOR\"\n",
    "\n",
    "    def analyze_text(self, text):\n",
    "        findings = []\n",
    "        # specific regex for full dates or standalone 1-3 digit numbers\n",
    "        for match in re.finditer(r'\\b(\\d{1,2}[./-]\\d{1,2}[./-]\\d{4}|\\d{1,3})\\b', text):\n",
    "            cand = match.group(0)\n",
    "            snip = text[max(0, match.start()-50):min(len(text), match.end()+50)].lower().replace(cand.lower(), \"<NUM>\")\n",
    "            vec = self.vectorizer.transform([snip])\n",
    "            \n",
    "            age_scr = float(np.max(cosine_similarity(vec, self.age_pos_vectors)))\n",
    "            dob_scr = float(np.max(cosine_similarity(vec, self.dob_pos_vectors)))\n",
    "            neg_scr = float(np.max(cosine_similarity(vec, self.neg_vectors)))\n",
    "            \n",
    "            best_pos = max(age_scr, dob_scr)\n",
    "            # High threshold to prevent generic counts (e.g. 2 children) from matching\n",
    "            if best_pos > self.threshold and best_pos > neg_scr:\n",
    "                age, is_date_format = self.calculate_age(cand)\n",
    "                # Determine label: use PII:BIRTHDAY if dob score is higher or it looks like a full date\n",
    "                label = self.get_pii_type(age, dob_scr > age_scr or is_date_format)\n",
    "                if label:\n",
    "                    findings.append({\n",
    "                        \"type\": label, \n",
    "                        \"text\": cand, \n",
    "                        \"start\": match.start(), \n",
    "                        \"end\": match.end(), \n",
    "                        \"confidence\": round(best_pos, 2)\n",
    "                    })\n",
    "        return findings\n",
    "\n",
    "# ==========================================\n",
    "# 5. UNIFIED PIPELINE (MERGED LOGIC)\n",
    "# ==========================================\n",
    "class UnifiedPIIPipeline:\n",
    "    def __init__(self):\n",
    "        print(\"Loading NLP Models...\")\n",
    "        try: self.nlp = spacy.load(\"de_core_news_lg\")\n",
    "        except:\n",
    "            from spacy.cli import download\n",
    "            download(\"de_core_news_lg\")\n",
    "            self.nlp = spacy.load(\"de_core_news_lg\")\n",
    "\n",
    "        self.medications = [\"ibuprofen\", \"aspirin\", \"paracetamol\", \"antibiotika\", \"insulin\"]\n",
    "        self.conditions = [\"kopfschmerzen\", \"migräne\", \"fieber\", \"husten\", \"diabetes\"]\n",
    "        self.stoplist = {\n",
    "            \"Morgen\", \"Heute\", \"Gestern\", \"Hallo\", \"Hi\", \"Hey\", \"Danke\", \"Bitte\", \"Grüße\", \n",
    "            \"Servus\", \"Moin\", \"Tschüss\", \"Bis\", \"Bald\", \"Ja\", \"Nein\", \"Vielleicht\", \n",
    "            \"Sehr\", \"Geehrte\", \"Damen\", \"Herren\", \"Liebe\", \"Lieber\", \"Herr\", \"Frau\", \n",
    "            \"Anwalt\", \"Arzt\", \"Ärztin\", \"Dr.\", \"Prof.\", \"Dipl.\", \"Ing.\"\n",
    "        }\n",
    "        \n",
    "        self.noun_blocklist = {\n",
    "            # Existing technical labels\n",
    "            \"Perso\", \"Ausweis\", \"Pass\", \"Konto\", \"Bank\", \"Iban\", \"Nummer\", \"Tel\", \"Handy\", \n",
    "            \"Telefon\", \"Email\", \"Mail\", \"Adresse\", \"Name\", \"Büro\", \"Handynummer\", \n",
    "            \"Steuer-ID\", \"Glückszahl\", \"SV-Nr\", \"SVN\", \"BIC\", \"SWIFT\", \"sorry\", \"thanks\", \"thank you\"\n",
    "            \n",
    "            # Relationship & Collective Terms (High FP in CSV)\n",
    "            \"Sohn\", \"Tochter\", \"Kind\", \"Enkel\", \"Enkelin\", \"Enkelkind\", \"Ehemann\", \"Ehefrau\", \n",
    "            \"Partner\", \"Partnerin\", \"Eltern\", \"Mutter\", \"Vater\", \"Bruder\", \"Schwester\", \n",
    "            \"Familie\", \"Ehepaar\", \"Fam\", \"Geschwister\", \"Freund\", \"Freundin\", \"Bekannte\",\n",
    "            \n",
    "            # Business & Document Labels\n",
    "            \"Rechnungsnummer\", \"Kundennummer\", \"Steuernummer\", \"Steuernr\", \"USt-IdNr\", \n",
    "            \"Mitgliedsnummer\", \"Versicherungsnummer\", \"Auftragsnummer\", \"Führerschein\", \n",
    "            \"Führerschein-Nr\", \"Reisepass\", \"Passnummer\", \"Vertrag\", \"Abrechnung\", \n",
    "            \"Lohnabrechnung\", \"Girocard\", \"Mastercard\", \"Visa\", \"Kreditkarte\",\n",
    "            \n",
    "            # Communication & Generic Contexts\n",
    "            \"Willkommensmail\", \"Sprachnachricht\", \"Chatverlauf\", \"Anfrage\", \"Antwort\", \n",
    "            \"Nachricht\", \"Zettel\", \"Vorlage\", \"Bericht\", \"Notiz\", \"Telefonnotiz\", \n",
    "            \"E-Mail\", \"Emailadresse\", \"Formular\", \"Personalbogen\", \"Ausschreibung\",\n",
    "            \n",
    "            # Verbs/Nouns often capitalized at sentence start\n",
    "            \"Formulieren\", \"Formulier\", \"Schreiben\", \"Helfen\", \"Antworten\", \"Zusammenfassen\", \n",
    "            \"Bearbeiten\", \"Erfassen\", \"Eintragen\", \"Frage\", \"Problem\", \"Hilfe\", \"Rückruf\",\n",
    "            \n",
    "            # Address/Location Parts\n",
    "            \"Allee\", \"Straße\", \"Strasse\", \"Weg\", \"Platz\", \"Ufer\", \"Wall\", \"Ring\", \"Gasse\", \n",
    "            \"Hof\", \"Garten\", \"Markt\", \"Hausnummer\", \"Hausnr\", \"PLZ\", \"Ort\", \"Stadt\", \n",
    "            \"Land\", \"Deutschland\", \"Berlin\", \"Hamburg\", \"München\", \"Köln\",\n",
    "            \"PLZ\", \"Postleitzahl\", \"Anschrift\", \"Wohnhaft\", \"Einsatzort\", \"Hausnummer\", \"Hausnr\", \"Ort\", \"Stadt\"\n",
    "        }\n",
    "\n",
    "        ruler = self.nlp.add_pipe(\"entity_ruler\", before=\"ner\")\n",
    "        patterns = []\n",
    "        for item in self.medications: patterns.append({\"label\": \"MEDICATION\", \"pattern\": [{\"LOWER\": item}]})\n",
    "        for item in self.conditions: patterns.append({\"label\": \"CONDITION\", \"pattern\": [{\"LOWER\": item}]})\n",
    "  \n",
    "        sfx = \"straße|strasse|str\\.|weg|platz|allee|damm|ring|gasse|ufer|chaussee|hof|garten|markt|zeile|wall|graben|kirchweg|landstraße|pfad|autobahn|bundesstraße\"\n",
    "        patterns.append({\"label\": \"ADDRESS_DETECTED\", \"pattern\": [{\"TEXT\": {\"REGEX\": f\"(?i).*({sfx})$\"}}, {\"TEXT\": {\"REGEX\": r\"^\\d\"}}]})\n",
    "        patterns.append({\"label\": \"POSTALCODE_DETECTED\", \"pattern\": [{\"LOWER\": \"plz\"}, {\"TEXT\": {\"REGEX\": r\"^\\d{5}$\"}}]})\n",
    "        ruler.add_patterns(patterns)\n",
    "\n",
    "        self.matcher = DependencyMatcher(self.nlp.vocab)\n",
    "        self.age_extractor = FastAgeExtractor(threshold=0.65)\n",
    "        self.iban_context_validator = IBANContextValidator()\n",
    "        self.card_context_validator = CardContextValidator()\n",
    "        self.phone_context_validator = PhoneContextValidator()\n",
    "        self.passport_context_validator = PassportContextValidator()\n",
    "        self.svn_context_validator = SVNContextValidator()\n",
    "        self.national_validator = NationalContextValidator()\n",
    "        self.tax_context_validator = TaxContextValidator()\n",
    "        self.bic_context_validator = BICContextValidator()\n",
    "        # Initializing Added Validators\n",
    "        self.insurance_validator = InsuranceContextValidator()\n",
    "        self.license_validator = DriverLicenseContextValidator()\n",
    "        self.plate_validator = DriverPlateContextValidator()\n",
    "        self.ust_validator = USTContextValidator()\n",
    "        self.pin_context_validator = PINContextValidator()\n",
    "        \n",
    "        self.regex_detector = RegexPIIDetector(\n",
    "            iban_validator=self.iban_context_validator, \n",
    "            card_validator=self.card_context_validator,\n",
    "            phone_validator=self.phone_context_validator,\n",
    "            passport_validator=self.passport_context_validator,\n",
    "            svn_validator=self.svn_context_validator,\n",
    "            national_validator=self.national_validator,\n",
    "            tax_validator=self.tax_context_validator,\n",
    "            bic_validator=self.bic_context_validator,\n",
    "            # Passing Added Validators\n",
    "            insurance_validator=self.insurance_validator,\n",
    "            license_validator=self.license_validator,\n",
    "            plate_validator=self.plate_validator,\n",
    "            ust_validator=self.ust_validator,\n",
    "            pin_validator=self.pin_context_validator\n",
    "        )\n",
    "\n",
    "\n",
    "    def _trim_entity_by_pos(self, ent):\n",
    "        \"\"\"Uses SpaCy POS tags to remove leading Verbs, Pronouns, and Prepositions.\"\"\"\n",
    "        start = 0\n",
    "        invalid_start_pos = {\"PRON\", \"VERB\", \"ADP\", \"DET\", \"AUX\", \"PART\", \"ADV\"}\n",
    "        \n",
    "        for token in ent:\n",
    "            # If word is a 'noise' grammatical type or in your blocklist, skip it\n",
    "            if token.pos_ in invalid_start_pos or token.lower_ in Validators.LOCATION_CONTEXT_WORDS:\n",
    "                start += 1\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        if start >= len(ent): return None\n",
    "        return ent[start:]\n",
    "\n",
    "    # --- ADD TO UnifiedPIIPipeline CLASS ---\n",
    "    def _clean_location_text(self, text: str) -> str:\n",
    "\n",
    "\n",
    "        if \"\\n\" in text:\n",
    "            # We only want the line containing the actual street suffix\n",
    "            suffixes_pattern = r\"(straße|strasse|str\\.|weg|platz|allee|damm|ring|gasse|ufer|chaussee|hof|garten|markt|zeile|wall|graben|kirchweg|landstraße|pfad|autobahn|bundesstraße)\"\n",
    "            lines = text.split(\"\\n\")\n",
    "            for line in lines:\n",
    "                if re.search(suffixes_pattern, line, re.IGNORECASE):\n",
    "                    text = line\n",
    "                    break\n",
    "        \"\"\"Aggressively prunes sentence context from the start of a location match.\"\"\"\n",
    "        # Common German street suffixes to anchor on\n",
    "        suffixes = r\"(straße|strasse|str\\.|weg|platz|allee|damm|ring|gasse|ufer|chaussee|hof|garten|markt|zeile|wall|graben|kirchweg|landstraße|pfad|autobahn|bundesstraße)\"\n",
    "        match = re.search(suffixes, text, re.IGNORECASE)\n",
    "        \n",
    "        if not match:\n",
    "            return text.strip(\" \\n\\r\\t.,:;-_\")\n",
    "\n",
    "        suffix_start = match.start()\n",
    "        # Look at the text before the suffix\n",
    "        pre_suffix = text[:suffix_start]\n",
    "        words = re.split(r'(\\s+)', pre_suffix)\n",
    "        \n",
    "        # Work backwards from the suffix to find the start of the actual street name\n",
    "        clean_words = []\n",
    "        # We typically want 1-2 words before the suffix (e.g., \"Theodor Heuss\" Allee)\n",
    "        # but we stop if we hit a 'context word' (like 'wohnt')\n",
    "        for i in range(len(words)-1, -1, -1):\n",
    "            word_clean = words[i].strip(\" \\n\\r\\t.,:;-_\").lower()\n",
    "            if not word_clean:\n",
    "                clean_words.insert(0, words[i])\n",
    "                continue\n",
    "            if word_clean in Validators.LOCATION_CONTEXT_WORDS or word_clean in self.noun_blocklist:\n",
    "                break\n",
    "            clean_words.insert(0, words[i])\n",
    "            # Limit to 3 words before suffix to avoid grabbing the whole sentence\n",
    "            if len([w for w in clean_words if w.strip()]) >= 3:\n",
    "                break\n",
    "                \n",
    "        # Join the kept words with the rest of the string (suffix + house number)\n",
    "        result = \"\".join(clean_words) + text[suffix_start:]\n",
    "        return result.strip(\" \\n\\r\\t.,:;-_\")\n",
    "\n",
    "\n",
    "\n",
    "    def _generate_token(self, pii_type, text_segment):\n",
    "        h = hashlib.md5(text_segment.lower().encode()).hexdigest()[:8]\n",
    "        return f\"[PII:{pii_type.replace(':', '_')}_ID_{h}]\"\n",
    "\n",
    "    def _analyze_person_entity(self, ent):\n",
    "        role_keywords = {\"Dr.\", \"Prof.\", \"Arzt\", \"Ärztin\", \"Herr\", \"Frau\", \"Anwalt\"}\n",
    "        detected_role = \"N/A\"\n",
    "        clean_name_parts = []\n",
    "        for token in ent:\n",
    "            if token.text in role_keywords: detected_role = token.text\n",
    "            else: clean_name_parts.append(token.text)\n",
    "        if detected_role == \"N/A\" and ent.start > 0:\n",
    "            prev_token = ent.doc[ent.start - 1]\n",
    "            if prev_token.text in role_keywords: detected_role = prev_token.text\n",
    "        clean_name = re.sub(r\"[^\\w\\s-]\", \"\", \" \".join(clean_name_parts).strip())\n",
    "        return clean_name or ent.text, detected_role\n",
    "\n",
    "    def process_batch(self, text_list: List[str]) -> List[Dict[str, Any]]:\n",
    "        results = []\n",
    "        for original_text in text_list:\n",
    "            start_time = time.time()\n",
    "            all_findings = []\n",
    "            occupied = set()\n",
    "\n",
    "            regex_detections = self.regex_detector.detect(original_text)\n",
    "            for f in regex_detections:\n",
    "                if f['type'] == \"PII:LOCATION:ADDRESS\":\n",
    "                    # Apply aggressive cleaning to the regex result\n",
    "                    cleaned = self._clean_location_text(f['text'])\n",
    "                    if not cleaned: continue\n",
    "                    \n",
    "                    # Recalculate positions based on the cleaned string\n",
    "                    offset = f['text'].find(cleaned)\n",
    "                    f['text'] = cleaned\n",
    "                    f['start'] += offset\n",
    "                    f['end'] = f['start'] + len(cleaned)\n",
    "                \n",
    "                # Double-check span isn't already occupied (prevents duplicate overlaps)\n",
    "                if any(i in occupied for i in range(f['start'], f['end'])):\n",
    "                    continue\n",
    "\n",
    "                all_findings.append(f)\n",
    "                occupied.update(range(f['start'], f['end']))\n",
    "\n",
    "            for f in self.age_extractor.analyze_text(original_text):\n",
    "                if not any(i in occupied for i in range(f['start'], f['end'])):\n",
    "                    all_findings.append(f); occupied.update(range(f['start'], f['end']))\n",
    "\n",
    "            doc = self.nlp(original_text)\n",
    "            for ent in doc.ents:\n",
    "                if any(i in occupied for i in range(ent.start_char, ent.end_char)): continue\n",
    "                label, txt = ent.label_, ent.text\n",
    "                if label in [\"MEDICATION\", \"CONDITION\"]:\n",
    "                    continue\n",
    "                    all_findings.append({\"type\": f\"MED:{label}\", \"text\": txt, \"start\": ent.start_char, \"end\": ent.end_char, \"confidence\": 0.9})\n",
    "                    occupied.update(range(ent.start_char, ent.end_char))\n",
    "\n",
    "                if label in [\"LOC\", \"GPE\", \"ADDRESS_DETECTED\", \"POSTALCODE_DETECTED\"]:\n",
    "                    # TRIM BY POS (The \"Stronger\" part)\n",
    "                    trimmed = self._trim_entity_by_pos(ent)\n",
    "                    if not trimmed: continue\n",
    "                    \n",
    "                    clean_txt = trimmed.text.strip(\" \\n\\r\\t.,:;-_\")\n",
    "                    s_char, e_char = trimmed.start_char, trimmed.end_char\n",
    "                    plz_match = re.search(r'\\b\\d{5}\\b', clean_txt)\n",
    "                    \n",
    "                    if label == \"POSTALCODE_DETECTED\" or re.fullmatch(r\"\\d{5}\", clean_txt):\n",
    "                        all_findings.append({\"type\": \"PII:LOCATION:POSTALCODE\", \"text\": clean_txt, \"start\": s_char, \"end\": e_char, \"confidence\": 0.98})\n",
    "                    elif label == \"ADDRESS_DETECTED\" or re.search(r\"\\d\", clean_txt):\n",
    "                        if plz_match and plz_match.group() != clean_txt:\n",
    "                            # SPLIT: Address and Postal Code\n",
    "                            plz_val = plz_match.group()\n",
    "                            plz_start = s_char + clean_txt.find(plz_val)\n",
    "                            all_findings.append({\"type\": \"PII:LOCATION:POSTALCODE\", \"text\": plz_val, \"start\": plz_start, \"end\": plz_start + 5, \"confidence\": 0.98})\n",
    "                            \n",
    "                            addr_part = clean_txt.replace(plz_val, \"\").strip(\" ,\")\n",
    "                            if addr_part:\n",
    "                                all_findings.append({\"type\": \"PII:LOCATION:ADDRESS\", \"text\": addr_part, \"start\": s_char, \"end\": s_char + len(addr_part), \"confidence\": 0.95})\n",
    "                        else:\n",
    "                            all_findings.append({\"type\": \"PII:LOCATION:ADDRESS\", \"text\": clean_txt, \"start\": s_char, \"end\": e_char, \"confidence\": 0.95})\n",
    "                    occupied.update(range(s_char, e_char))\n",
    "\n",
    "                    \n",
    "                elif label in [\"PER\", \"PER_STRONG\"]:\n",
    "                    clean_name, role = self._analyze_person_entity(ent)\n",
    "                    if clean_name.title() not in self.stoplist and not any(p.strip().title() in self.noun_blocklist for p in clean_name.split()):\n",
    "                        all_findings.append({\"type\": \"PII:PERSON\", \"text\": txt, \"start\": ent.start_char, \"end\": ent.end_char, \"confidence\": 0.9})\n",
    "                        occupied.update(range(ent.start_char, ent.end_char))\n",
    "\n",
    "            all_findings.sort(key=lambda x: x['start'], reverse=True)\n",
    "            masked_text = original_text\n",
    "            for f in all_findings:\n",
    "                token = self._generate_token(f['type'], f['text'])\n",
    "                masked_text = masked_text[:f['start']] + token + masked_text[f['end']:]\n",
    "                f['token'] = token\n",
    "\n",
    "            results.append({\"has_pii\": len(all_findings) > 0, \"detections\": all_findings[::-1], \"anonymized_text\": masked_text, \"processing_time_ms\": int((time.time() - start_time) * 1000)})\n",
    "        return results\n",
    "\n",
    "# ==========================================\n",
    "# 6. EXECUTION\n",
    "# ==========================================\n",
    "if __name__ == \"__main__\":\n",
    "    pipeline = UnifiedPIIPipeline()\n",
    "    samples = [\n",
    "        \"BIC: GIBACZPX\",\n",
    "        \"Kowalski wechselt zur Barmer.\", \n",
    "        \"USt-IdNr. DE287654321\",\n",
    "        \"Führerschein AA3EFB51059\",\n",
    "        \"Das Kennzeichen ist DO-RB 472\",\n",
    "        \"Überweisung an Techniker Krankenkasse, SWIFT: NOLADE21KIE\",\n",
    "        \"gültig bis 03/28, cvv 847 anzahlung für badezimmer-renovierung: 500€\"\n",
    "    ]\n",
    "    output = pipeline.process_batch(samples)\n",
    "    print(json.dumps(output, indent=4, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75aa0f0-7ad6-4be4-9cdc-3c6a05fc2566",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
