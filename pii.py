# -*- coding: utf-8 -*-
"""final_pii

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1y7woIYUvOp0BnuyD3fIixMv3ypLsg18X
"""

'''Hi,
I implemented the solution using a "best-of-both-worlds" approach—combining highly optimized Regex for strict patterns and AI models for context. I just wanted to highlight a few specific logic decisions to make sure they align with your needs:

•⁠  ⁠*Strict Financial Validation:* I implemented mathematical checksums (Modulo 97 for IBANs and Luhn Algorithm for Credit Cards). This means if a user types a *fake or mathematically invalid* IBAN, the system will *not* mask it (it ignores it to prevent false positives). Please let me know if you would prefer a "looser" detection that masks anything resembling an IBAN, even if it's invalid.
•⁠  ⁠*Location Context:* As requested, the system is currently configured to *keep city names* (e.g., "Hamburg", "Berlin") visible to preserve context, while strictly masking specific street addresses. Tell me if this needs to be changed.
Let me know if you’d like to adjust any of these parameters!
The timetaken for per sentence on an average is lower than 100ms on a CPU. I tried to integrate the best things possible with the time constraint
Best,
Ojaswini Sharma'''

import re
import time
import json
import hashlib
import numpy as np
import spacy
from datetime import datetime
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from spacy.matcher import DependencyMatcher
from typing import List, Dict, Any, Set

# ==========================================
# 1. USER PROVIDED VALIDATORS (STRICT LOGIC)
# ==========================================
class Validators:
    IGNORED_FILE_EXTENSIONS = {
        'pdf', 'jpg', 'png', 'gif', 'doc', 'docx', 'xls', 'xlsx',
        'txt', 'zip', 'rar', 'exe', 'mp3', 'mp4', 'json', 'xml', 'js', 'py'
    }

    SAFE_TLDS = {
        'com', 'net', 'org', 'info', 'biz', 'co', 'io', 'me', 'edu', 'gov', 'int', 'mil',
        'de', 'at', 'ch', 'eu', 'nl', 'fr', 'uk', 'be', 'dk', 'no', 'se', 'fi', 'pl', 'it', 'es',
        'app', 'dev', 'ai', 'cloud', 'tech', 'digital', 'studio', 'online', 'shop', 'store',
        'berlin', 'hamburg', 'koeln', 'bayern'
    }

    @staticmethod
    def normalize(text: str) -> str:
        """Removes spaces, dashes, dots, parens for validation."""
        return re.sub(r"[\s\-\./\(\)]", "", text)

    # --- PHONE VALIDATION ---
    @staticmethod
    def validate_phone(text: str) -> bool:
        clean = Validators.normalize(text)
        if len(clean) < 7: return False
        if re.search(r"\d{2}\.\d{2}\.\d{4}", text): return False # Date
        if len(set(clean)) == 1: return False # 0000000
        if re.search(r"\b\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}\b", text): return False # IP
        return True

    # --- EMAIL VALIDATION ---
    @staticmethod
    def validate_email(text: str) -> bool:
        if "@" not in text: return False
        parts = text.split('.')
        return len(parts[-1]) >= 2

    # --- URL VALIDATION ---
    @staticmethod
    def validate_url(text: str) -> bool:
        if "@" in text: return False
        if text.lower().startswith(('http:', 'https:', 'www.')):
            return True

        # Naked Domain Logic
        domain_part = text.split('/')[0]
        parts = domain_part.split('.')
        if len(parts) < 2: return False

        # Check if the last part OR second-to-last part (co.uk) is safe
        valid_tld = False
        for part in parts[-2:]:
            if part.lower() in Validators.SAFE_TLDS:
                valid_tld = True
                break

        if not valid_tld: return False

        # Extra file check on the very last part
        last = parts[-1].lower()
        if len(last) < 2 or last.isdigit() or last in Validators.IGNORED_FILE_EXTENSIONS:
            return False

        return True

    # --- IBAN VALIDATION (MOD 97 CHECK) ---
    @staticmethod
    def validate_iban(text: str) -> bool:
        clean = Validators.normalize(text).upper()
        if len(clean) < 15 or len(clean) > 34: return False
        try:
            rearranged = clean[4:] + clean[:4]
            numeric_iban = ""
            for char in rearranged:
                if char.isalpha():
                    numeric_iban += str(ord(char) - 55)
                elif char.isdigit():
                    numeric_iban += char
                else:
                    return False
            return int(numeric_iban) % 97 == 1
        except ValueError:
            return False

    # --- CARD VALIDATION (LUHN CHECK) ---
    @staticmethod
    def validate_card(text: str) -> bool:
        clean = Validators.normalize(text)
        if not clean.isdigit(): return False
        if not (13 <= len(clean) <= 19): return False
        digits = [int(d) for d in clean]
        checksum = 0
        for i, digit in enumerate(reversed(digits)):
            if i % 2 == 1:
                doubled = digit * 2
                checksum += doubled if doubled < 10 else doubled - 9
            else:
                checksum += digit
        return checksum % 10 == 0

    # --- ID VALIDATION ---
    @staticmethod
    def validate_id(text: str, id_type: str) -> bool:
        clean = Validators.normalize(text)
        if id_type == "PII:ID:TAX": return len(clean) == 11 and clean.isdigit()
        if id_type == "PII:ID:SSN": return 9 <= len(clean) <= 15
        if id_type == "PII:ID:PASSPORT": return 6 <= len(clean) <= 12
        if id_type == "PII:ID:DRIVERLICENSE":
            # STRICT CHECK: Must be 11 chars AND contain at least one digit
            if len(clean) != 11: return False
            if not any(char.isdigit() for char in clean): return False # Rejects "Handynummer"
            return True
        if id_type == "PII:ID:NATIONAL": return 6 <= len(clean) <= 12
        return True

# ==========================================
# 2. USER PROVIDED DETECTOR
# ==========================================
class RegexPIIDetector:
    def __init__(self):
        self.patterns = {
            "FINANCIAL:IBAN": re.compile(r"""
                \b
                [A-Z]{2}                  # Country Code
                [\s\.\-]* \d{2}           # Check Digits
                (?:[\s\.\-]*[A-Z0-9])+    # Body (Greedy)
            """, re.VERBOSE | re.IGNORECASE),

            "FINANCIAL:CARD": re.compile(r"""
                \b
                [1-9]                     # Start with non-zero
                (?:[\s\-\–\/\.]*\d){12,18} # 12-18 more digits
                \b
            """, re.VERBOSE),

            # UPDATED URL REGEX
            "CONTACT:URL": re.compile(r"""
                \b
                (?:
                    (?:https?://|www\.)
                    [\w\-\.\/%\+~=\?&]+
                    |
                    [\w\-]+(?:\.[a-zA-Z]{2,})+  # Fixed: Repeated group for compound TLDs
                    (?:/[\w\-\.\/%\+~=\?&]*)?
                )
            """, re.VERBOSE | re.IGNORECASE),

            "CONTACT:PHONE": re.compile(r"""
                (?<!\d)(?:(?:(?:\+|00)49)[\s\-\./]*(?:(?:\(0\)|0)[\s\-\./]*)?(?:(?:\([1-9]\d{1,5}\)|[1-9]\d{1,5}))|(?:(?:\(0[\s\-\./]*[1-9]\d{1,5}\)|0[\s\-\./]*[1-9]\d{1,5})))(?:[\s\-\./]*\d){3,15}
            """, re.VERBOSE),

            "CONTACT:EMAIL": re.compile(r"\b[\w.+-]+@[\w-]+(?:\.[a-zA-Z-]{2,})+", re.IGNORECASE),

            # ID Patterns
            "PII:ID:TAX": re.compile(r"\b[1-9]\d{10}\b"),
            "PII:ID:SSN": re.compile(r"\b\d{2}[\s]*\d{6}[\s]*[A-Z][\s]*\d{3}\b", re.IGNORECASE),
            "PII:ID:DRIVERLICENSE": re.compile(r"\b(?=.*\d)(?=.*[A-Z])[A-Z0-9]{11}\b", re.IGNORECASE),
            "PII:ID:PASSPORT": re.compile(r"\b[C-Z0-9][0-9A-Z]{8}\b"),
            "PII:ID:NATIONAL": re.compile(r"\b[L-Z0-9][0-9A-Z]{8,9}\b"),
        }

    def _validate_match(self, pii_type: str, text: str) -> bool:
        if pii_type.startswith("PII:ID"): return Validators.validate_id(text, pii_type)
        if pii_type == "CONTACT:URL": return Validators.validate_url(text)
        elif pii_type == "CONTACT:PHONE": return Validators.validate_phone(text)
        elif pii_type == "CONTACT:EMAIL": return Validators.validate_email(text)
        elif pii_type == "FINANCIAL:IBAN": return Validators.validate_iban(text)
        elif pii_type == "FINANCIAL:CARD": return Validators.validate_card(text)
        return True

    def _resolve_conflicts(self, detections: List[Dict]) -> List[Dict]:
        if not detections: return []

        priority = {
            "FINANCIAL:IBAN": 100,
            "FINANCIAL:CARD": 95,
            "PII:ID:SSN": 90,
            "PII:ID:DRIVERLICENSE": 85,
            "PII:ID:PASSPORT": 85,
            "PII:ID:NATIONAL": 85,
            "CONTACT:EMAIL": 80,
            "PII:ID:TAX": 75,
            "CONTACT:URL": 50,
            "CONTACT:PHONE": 10
        }

        sorted_dets = sorted(detections, key=lambda x: (-priority.get(x['type'], 0), -(x['end'] - x['start'])))
        final, occupied = [], set()
        for det in sorted_dets:
            r = range(det['start'], det['end'])
            if not any(i in occupied for i in r):
                final.append(det)
                occupied.update(r)
        return sorted(final, key=lambda x: x['start'])

    def detect(self, text: str) -> List[Dict]:
        # NOTE: Adapted to return list directly for the Unified Pipeline
        raw_detections = []
        for pii_type, pattern in self.patterns.items():
            for match in pattern.finditer(text):
                candidate = match.group()
                clean_text = candidate.rstrip(".,;:!?")

                # Stripping
                if pii_type == "CONTACT:URL" and "/" not in clean_text:
                    while clean_text and clean_text[-1].isdigit(): clean_text = clean_text[:-1]
                if pii_type == "FINANCIAL:IBAN":
                    while len(clean_text) > 14:
                        if Validators.validate_iban(clean_text): break
                        clean_text = clean_text[:-1].rstrip(" .-")
                    if len(clean_text) <= 14: clean_text = candidate.rstrip(".,;:!?")

                end_pos = match.start() + len(clean_text)

                if self._validate_match(pii_type, clean_text):
                    raw_detections.append({
                        "type": pii_type,
                        # Token generation moved to pipeline for consistency, but type needed
                        "text": clean_text,
                        "start": match.start(),
                        "end": end_pos,
                        "confidence": 1.0
                    })

        return self._resolve_conflicts(raw_detections)

# ==========================================
# 3. AGE EXTRACTION (CONTEXTUAL)
# ==========================================
class FastAgeExtractor:
    def __init__(self, threshold=0.30):
        self.threshold = threshold
        self.current_year = datetime.now().year
        self.pos_anchors = [
            "Ich bin <NUM> Jahre alt", "Er ist <NUM> geworden", "Sie ist <NUM>",
            "Mein Alter ist <NUM>", "Das Kind ist <NUM>", "Ein <NUM>-Jähriger",
            "Mit <NUM> Jahren", "Geboren am <NUM>", "Mein Geburtsdatum ist <NUM>",
            "Baujahr <NUM>", "Jahrgang <NUM>", "Geburtstag am <NUM>",
            "Nächstes Jahr werde ich <NUM>", "Der ist schon <NUM>"
        ]
        self.neg_anchors = [
            "Das kostet <NUM> Euro", "Preis <NUM> EUR", "Ich habe <NUM> Äpfel",
            "In <NUM> Minuten", "Hausnummer <NUM>", "Seite <NUM>", "Um <NUM> Uhr",
            "<NUM> Prozent", "Gewicht <NUM> kg", "PLZ <NUM>", "Verspätung <NUM>",
            "Nummer <NUM>", "Platz <NUM>", "Größe <NUM>"
        ]
        all_anchors = self.pos_anchors + self.neg_anchors
        self.vectorizer = TfidfVectorizer(analyzer='word', ngram_range=(1, 2))
        self.vectorizer.fit(all_anchors)
        self.pos_vectors = self.vectorizer.transform(self.pos_anchors)
        self.neg_vectors = self.vectorizer.transform(self.neg_anchors)

    def calculate_age(self, value_str):
        if re.search(r'[./-]', value_str):
            for fmt in ("%d.%m.%Y", "%d/%m/%Y", "%d-%m-%Y"):
                try:
                    dob = datetime.strptime(value_str, fmt)
                    age = self.current_year - dob.year - ((datetime.now().month, datetime.now().day) < (dob.month, dob.day))
                    return age
                except ValueError: continue
            return None
        try:
            val = int(value_str)
            if 1900 < val <= self.current_year: return self.current_year - val
            return val
        except ValueError: return None

    def get_pii_type(self, age):
        if age is None: return "AGE:UNKNOWN"
        if age > 120: return None
        if age < 12: return "AGE:CHILD"
        elif 12 <= age <= 17: return "AGE:TEEN"
        elif 18 <= age <= 64: return "AGE:ADULT"
        else: return "AGE:SENIOR"

    def analyze_text(self, text):
        findings = []
        regex_pattern = r'\b(\d{1,2}[./-]\d{1,2}[./-]\d{4}|\d{1,4})\b'
        for match in re.finditer(regex_pattern, text):
            candidate_val = match.group(0)
            start, end = match.span()
            window_start = max(0, start - 50)
            window_end = min(len(text), end + 50)
            context_snippet = text[window_start:window_end]
            masked_snippet = context_snippet.replace(candidate_val, "<NUM>", 1)
            input_vector = self.vectorizer.transform([masked_snippet])
            pos_score = float(np.max(cosine_similarity(input_vector, self.pos_vectors)))
            neg_score = float(np.max(cosine_similarity(input_vector, self.neg_vectors)))
            if pos_score > self.threshold and pos_score > neg_score:
                age = self.calculate_age(candidate_val)
                type_label = self.get_pii_type(age)
                if type_label:
                    findings.append({
                        "type": type_label, "text": candidate_val, "start": start, "end": end,
                        "confidence": round(pos_score, 2), "metadata": {"calculated_age": age}
                    })
        return findings

# ==========================================
# 4. UNIFIED PIPELINE (MERGED LOGIC)
# ==========================================
class UnifiedPIIPipeline:
    def __init__(self):
        print("Loading NLP Models...")
        try: self.nlp = spacy.load("de_core_news_lg")
        except:
            from spacy.cli import download
            download("de_core_news_lg")
            self.nlp = spacy.load("de_core_news_lg")

        self.medications = ["ibuprofen", "aspirin", "paracetamol", "antibiotika", "tabletten", "medis", "salbe", "insulin"]
        self.conditions = ["kopfschmerzen", "migräne", "fieber", "husten", "schmerzen", "grippe", "depression", "diabetes"]
        self.procedures = ["mrt", "röntgen", "operation", "op", "checkup", "blutabnahme", "reha", "chemo"]
        # 1. ALLOWLIST
        self.famous_people = {
        }

        # 2. STOPLIST
        self.stoplist = {
            "Morgen", "Heute", "Gestern", "Abend", "Nabend",
            "Moin", "Moin Moin", "Moin moin", "Hallo", "Servus", "Hi", "Hey",
            "Tach", "Jo", "Ja", "Nein", "Danke", "Bitte", "Grüße"
        }

        # 3. NOUN BLOCKLIST (False Positives)
        self.noun_blocklist = {
            "Perso", "Ausweis", "Pass", "Reisepass", "Führerschein", "Fahrerlaubnis",
            "Konto", "Bank", "Iban", "Nummer", "Nr", "Tel", "Handy", "Telefon",
            "Link", "Info", "Infos", "Email", "Mail", "Adresse", "Steuerid",
            "Steuernummer", "Id", "Kind", "Tochter", "Sohn", "Vater", "Mutter",
            "Oma", "Opa", "Mann", "Frau", "Anmeldung", "Antrag", "Rente",
            "Senior", "Rentner", "Rentnerin", "Karte", "Kreditkarte", "Name",
            "Büro", "Ferienbetreuung", "Schwimmschule", "Gymnasium", "Monatskarte",
            "Hausnotruf", "Vollmacht", "Ferienjob", "Meine", "Handynummer","Perso", "Ausweis", "Pass", "Konto", "Bank", "Nummer", "Tel", "Email", "Adresse", "Name", "Büro",
            "Handynummer", "Steuer-ID", "Steuerid", "Reisepassnummer", "Führerschein", "Glückszahl", "Steuer-ID", "Hi", "Id",
            "perso", "ausweis", "pass", "konto", "bank", "nummer", "tel", "email", "adresse", "name", "büro",
            "handynummer", "steuer-id", "steuerid", "reisepassnummer", "führerschein", "glückszahl"
        }

        ruler = self.nlp.add_pipe("entity_ruler", before="ner")
        patterns = []
        for item in self.medications: patterns.append({"label": "MEDICATION", "pattern": [{"LOWER": item}]})
        for item in self.conditions: patterns.append({"label": "CONDITION", "pattern": [{"LOWER": item}]})
        for item in self.procedures: patterns.append({"label": "PROCEDURE", "pattern": [{"LOWER": item}]})

        sfx = "straße|strasse|str.|weg|platz|allee|damm|ring|gasse|ufer|chaussee|hof|garten|markt|zeile|wall"
        titles = "Herr|Frau|Dr.|Prof.|Arzt|Ärztin|Familie|Kanzler|Minister|Anwalt|Geschäftsführer"

        patterns.extend([
            {"label": "ADDRESS_DETECTED", "pattern": [{"TEXT": {"REGEX": f"(?i).+({sfx})$"}}, {"TEXT": {"REGEX": r"^\d"}}]},
            {"label": "ADDRESS_DETECTED", "pattern": [{"IS_TITLE": True}, {"TEXT": {"REGEX": f"(?i)^({sfx})$"}}, {"TEXT": {"REGEX": r"^\d"}}]},
            {"label": "PER_STRONG", "pattern": [{"TEXT": {"REGEX": f"^({titles})$"}}, {"IS_TITLE": True}, {"IS_TITLE": True, "OP": "?"}]}
        ])
        ruler.add_patterns(patterns)

        self.matcher = DependencyMatcher(self.nlp.vocab)
        context_pattern = [
            {"RIGHT_ID": "verb", "RIGHT_ATTRS": {"POS": "VERB"}},
            {"LEFT_ID": "verb", "REL_OP": ">", "RIGHT_ID": "med", "RIGHT_ATTRS": {"ENT_TYPE": "MEDICATION"}},
            {"LEFT_ID": "verb", "REL_OP": ">", "RIGHT_ID": "prep", "RIGHT_ATTRS": {"LOWER": {"IN": ["wegen", "gegen", "für", "bei"]}}},
            {"LEFT_ID": "prep", "REL_OP": ">", "RIGHT_ID": "cond", "RIGHT_ATTRS": {"ENT_TYPE": "CONDITION"}}
        ]
        self.matcher.add("MED_CONTEXT", [context_pattern])

        self.age_extractor = FastAgeExtractor(threshold=0.60)
        self.regex_detector = RegexPIIDetector()

    def _generate_token(self, pii_type, text_segment):
        h = hashlib.md5(text_segment.lower().encode()).hexdigest()[:8]
        clean_type = pii_type.replace(":", "_")
        return f"[PII:{clean_type}_ID_{h}]"

    def _analyze_person_entity(self, ent):
        role_keywords = {"Dr.", "Prof.", "Arzt", "Ärztin", "Herr", "Frau", "Anwalt"}
        detected_role = "N/A"
        clean_name_parts = []
        for token in ent:
            if token.text in role_keywords: detected_role = token.text
            else: clean_name_parts.append(token.text)

        if detected_role == "N/A" and ent.start > 0:
            prev_token = ent.doc[ent.start - 1]
            if prev_token.text in role_keywords: detected_role = prev_token.text

        clean_name = " ".join(clean_name_parts).strip()
        if not clean_name: clean_name = ent.text
        clean_name = re.sub(r"[^\w\s-]", "", clean_name)
        return clean_name, detected_role

    def process_batch(self, text_list: List[str]) -> List[Dict[str, Any]]:
        results = []

        for original_text in text_list:
            start_time = time.time()
            all_findings = []
            occupied_indices: Set[int] = set()

            # -----------------------------------------------------------
            # 1. RUN REGEX DETECTOR (HIGHEST PRIORITY)
            # -----------------------------------------------------------
            # This detects Emails, IBANs, URLs, IDs based on YOUR strict validators.
            regex_findings = self.regex_detector.detect(original_text)
            all_findings.extend(regex_findings)

            # Occupy indices to prevent Spacy from breaking them
            for f in regex_findings:
                for i in range(f['start'], f['end']): occupied_indices.add(i)

            # -----------------------------------------------------------
            # 2. RUN AGE EXTRACTOR
            # -----------------------------------------------------------
            age_findings = self.age_extractor.analyze_text(original_text)
            for f in age_findings:
                is_overlap = any(i in occupied_indices for i in range(f['start'], f['end']))
                if not is_overlap:
                    all_findings.append(f)
                    for i in range(f['start'], f['end']): occupied_indices.add(i)

            # -----------------------------------------------------------
            # 3. RUN NLP (Medical, Names, Locations)
            # -----------------------------------------------------------
            doc = self.nlp(original_text)

            matches = self.matcher(doc)
            for match_id, token_ids in matches:
                med_token = doc[token_ids[1]]
                cond_token = doc[token_ids[3]]

                if med_token.idx not in occupied_indices:
                    all_findings.append({
                        "type": "MED:MEDICATION", "text": med_token.text, "start": med_token.idx,
                        "end": med_token.idx + len(med_token.text), "confidence": 1.0, "metadata": {"context": "linked"}
                    })
                    for i in range(med_token.idx, med_token.idx + len(med_token.text)): occupied_indices.add(i)

                if cond_token.idx not in occupied_indices:
                    all_findings.append({
                        "type": "MED:CONDITION", "text": cond_token.text, "start": cond_token.idx,
                        "end": cond_token.idx + len(cond_token.text), "confidence": 1.0, "metadata": {"context": "linked"}
                    })
                    for i in range(cond_token.idx, cond_token.idx + len(cond_token.text)): occupied_indices.add(i)

            for ent in doc.ents:
                start_char, end_char = ent.start_char, ent.end_char
                if any(i in occupied_indices for i in range(start_char, end_char)): continue

                label = ent.label_
                text_chunk = ent.text

                if label in ["MEDICATION", "CONDITION", "PROCEDURE"]:
                    all_findings.append({"type": f"MED:{label}", "text": text_chunk, "start": start_char, "end": end_char, "confidence": 0.9})
                    for i in range(start_char, end_char): occupied_indices.add(i)

                elif label in ["LOC", "GPE", "ADDRESS_DETECTED"]:
                    is_address = (label == "ADDRESS_DETECTED")
                    if not is_address and re.search(r"\d", text_chunk): is_address = True
                    if is_address:
                        all_findings.append({"type": "LOCATION:ADDRESS", "text": text_chunk, "start": start_char, "end": end_char, "confidence": 0.95})
                        for i in range(start_char, end_char): occupied_indices.add(i)

                elif label in ["PER", "PER_STRONG"]:
                    clean_name, role = self._analyze_person_entity(ent)
                    if clean_name in self.famous_people: continue
                    if clean_name.title() in self.stoplist or text_chunk in self.stoplist: continue
                    if "moin" in text_chunk.lower() and role == "N/A": continue
                    is_blocked = False
                    for part in clean_name.split():
                        if part.strip().title() in self.noun_blocklist: is_blocked = True
                    if not is_blocked:
                        all_findings.append({"type": "PERSON", "text": text_chunk, "start": start_char, "end": end_char, "confidence": 0.9})
                        for i in range(start_char, end_char): occupied_indices.add(i)

            # -----------------------------------------------------------
            # 4. CONSTRUCT OUTPUT
            # -----------------------------------------------------------
            all_findings.sort(key=lambda x: x['start'], reverse=True)

            final_detections = []
            masked_text = original_text

            for f in all_findings:
                unique_token = self._generate_token(f['type'], f['text'])
                try:
                    masked_text = masked_text[:f['start']] + unique_token + masked_text[f['end']:]
                except IndexError: continue

                det = {
                    "type": f['type'],
                    "token": unique_token,
                    "text": f['text'],
                    "start": f['start'],
                    "end": f['end'],
                    "confidence": f['confidence']
                }
                if "metadata" in f: det["metadata"] = f["metadata"]
                final_detections.append(det)

            elapsed_ms = int((time.time() - start_time) * 1000)
            results.append({
                "has_pii": len(final_detections) > 0,
                "detections": final_detections[::-1],
                "anonymized_text": masked_text,
                "processing_time_ms": elapsed_ms
            })

        return results

# ==========================================
# 5. EXECUTION
# ==========================================
if __name__ == "__main__":
    pipeline = UnifiedPIIPipeline()
    samples = []

    output = pipeline.process_batch(samples)
    print(json.dumps(output, indent=4, ensure_ascii=False))

