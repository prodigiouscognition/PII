{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc146bfd-ccb8-4284-af0f-a768843f7acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import json\n",
    "import hashlib\n",
    "import numpy as np\n",
    "import spacy\n",
    "from datetime import datetime\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from spacy.matcher import DependencyMatcher\n",
    "from typing import List, Dict, Any, Set\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64a58a4d-dcbf-42a4-8f60-f7d286209618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading NLP Models...\n",
      "[\n",
      "    {\n",
      "        \"has_pii\": true,\n",
      "        \"detections\": [\n",
      "            {\n",
      "                \"type\": \"PERSON\",\n",
      "                \"text\": \"Personalausweis\",\n",
      "                \"start\": 0,\n",
      "                \"end\": 15,\n",
      "                \"confidence\": 0.9,\n",
      "                \"token\": \"[PII:PERSON_ID_96fb3813]\"\n",
      "            },\n",
      "            {\n",
      "                \"type\": \"PII:ID:NATIONAL\",\n",
      "                \"text\": \"L12345678\",\n",
      "                \"start\": 24,\n",
      "                \"end\": 33,\n",
      "                \"confidence\": 1.0,\n",
      "                \"token\": \"[PII:PII_ID_NATIONAL_ID_1799e855]\"\n",
      "            }\n",
      "        ],\n",
      "        \"anonymized_text\": \"[PII:PERSON_ID_96fb3813] Nummer: [PII:PII_ID_NATIONAL_ID_1799e855]\",\n",
      "        \"processing_time_ms\": 18\n",
      "    },\n",
      "    {\n",
      "        \"has_pii\": true,\n",
      "        \"detections\": [\n",
      "            {\n",
      "                \"type\": \"PII:ID:PASSPORT\",\n",
      "                \"text\": \"FE893746\",\n",
      "                \"start\": 19,\n",
      "                \"end\": 27,\n",
      "                \"confidence\": 1.0,\n",
      "                \"token\": \"[PII:PII_ID_PASSPORT_ID_72772d8c]\"\n",
      "            }\n",
      "        ],\n",
      "        \"anonymized_text\": \"Reisepass Ukraine: [PII:PII_ID_PASSPORT_ID_72772d8c]\",\n",
      "        \"processing_time_ms\": 2\n",
      "    },\n",
      "    {\n",
      "        \"has_pii\": true,\n",
      "        \"detections\": [\n",
      "            {\n",
      "                \"type\": \"PII:ID:SVN\",\n",
      "                \"text\": \"11240601S003\",\n",
      "                \"start\": 23,\n",
      "                \"end\": 35,\n",
      "                \"confidence\": 1.0,\n",
      "                \"token\": \"[PII:PII_ID_SVN_ID_19ebb992]\"\n",
      "            }\n",
      "        ],\n",
      "        \"anonymized_text\": \"Meine SV-Nummer lautet [PII:PII_ID_SVN_ID_19ebb992]\",\n",
      "        \"processing_time_ms\": 4\n",
      "    },\n",
      "    {\n",
      "        \"has_pii\": true,\n",
      "        \"detections\": [\n",
      "            {\n",
      "                \"type\": \"CONTACT:PHONE\",\n",
      "                \"text\": \"0228 9182736\",\n",
      "                \"start\": 32,\n",
      "                \"end\": 44,\n",
      "                \"confidence\": 1.0,\n",
      "                \"token\": \"[PII:CONTACT_PHONE_ID_53c73491]\"\n",
      "            }\n",
      "        ],\n",
      "        \"anonymized_text\": \"Ansprechpartner: Herr Brinkmann [PII:CONTACT_PHONE_ID_53c73491]\",\n",
      "        \"processing_time_ms\": 5\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import hashlib\n",
    "import numpy as np\n",
    "import spacy\n",
    "from datetime import datetime\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from spacy.matcher import DependencyMatcher\n",
    "from typing import List, Dict, Any, Set\n",
    "\n",
    "# ==========================================\n",
    "# 1. USER PROVIDED VALIDATORS (STRICT LOGIC)\n",
    "# ==========================================\n",
    "class Validators:\n",
    "    IGNORED_FILE_EXTENSIONS = {\n",
    "        'pdf', 'jpg', 'png', 'gif', 'doc', 'docx', 'xls', 'xlsx',\n",
    "        'txt', 'zip', 'rar', 'exe', 'mp3', 'mp4', 'json', 'xml', 'js', 'py'\n",
    "    }\n",
    "\n",
    "    # Common non-PII words that match ID patterns (Rubbish filter)\n",
    "    GERMAN_RUBBISH = {\n",
    "        \"kannst\", \"helfen\", \"beantworten\", \"verstopft\", \"freundlich\", \"formulieren\", \n",
    "        \"morgen\", \"zwischen\", \"kommen\", \"folgene\", \"reingekommen\", \"antwort\", \n",
    "        \"geehrte\", \"herren\", \"mitarbeiter\", \"leider\", \"melden\", \"nettes\", \n",
    "        \"brauche\", \"vorlage\", \"wohnhaft\", \"heisst\", \"nachricht\", \"dringend\",\n",
    "        \"stunde\", \"vertrag\", \"tabelle\", \"eintrag\", \"arbeitge\", \"adresse\"\n",
    "    }\n",
    "\n",
    "    SAFE_TLDS = {\n",
    "        'com', 'net', 'org', 'info', 'biz', 'co', 'io', 'me', 'edu', 'gov', 'int', 'mil',\n",
    "        'de', 'at', 'ch', 'eu', 'nl', 'fr', 'uk', 'be', 'dk', 'no', 'se', 'fi', 'pl', 'it', 'es',\n",
    "        'app', 'dev', 'ai', 'cloud', 'tech', 'digital', 'studio', 'online', 'shop', 'store',\n",
    "        'berlin', 'hamburg', 'koeln', 'bayern'\n",
    "    }\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize(text: str) -> str:\n",
    "        \"\"\"Removes spaces, dashes, dots, parens for validation.\"\"\"\n",
    "        return re.sub(r\"[\\s\\-\\./\\(\\)\\\\]\", \"\", text)\n",
    "\n",
    "    @staticmethod\n",
    "    def fix_common_typos(text: str) -> str:\n",
    "        \"\"\"Fixes common OCR/Typo errors specifically for financial strings.\"\"\"\n",
    "        return text.upper().replace('O', '0').replace('I', '1').replace('S', '5')\n",
    "\n",
    "    # --- PHONE VALIDATION ---\n",
    "    @staticmethod\n",
    "    def validate_phone(text: str) -> bool:\n",
    "        if re.search(r\"\\d{2}\\.\\d{2}\\.\\-\\d{2}\\.\\d{2}\", text): return False\n",
    "        if re.search(r\"\\d{1,2}[\\.\\/]\\d{1,2}[\\.\\/]\\d{2,4}\", text): return False\n",
    "        \n",
    "        clean = text.lower().replace('o', '0').replace('l', '1')\n",
    "        clean = re.sub(r\"onal|abortel\", \"\", clean)\n",
    "        clean = re.sub(r\"[\\s\\-\\./\\(\\)\\\\]\", \"\", clean)\n",
    "        \n",
    "        if len(clean) < 7 or len(clean) > 15: return False\n",
    "        if len(clean) == 8 and ('.' in text or '/' in text):\n",
    "            if re.search(r\"[01]\\d[\\.\\/][12]\\d{3}\", text) or re.search(r\"[12]\\d{3}[\\.\\/][01]\\d\", text):\n",
    "                return False\n",
    "        if len(set(clean)) <= 2 and len(clean) > 9: return False \n",
    "        if re.search(r\"\\b\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\b\", text): return False \n",
    "        return True\n",
    "\n",
    "    # --- EMAIL VALIDATION ---\n",
    "    @staticmethod\n",
    "    def validate_email(text: str) -> bool:\n",
    "        if \"@\" not in text: return False\n",
    "        parts = text.split('.')\n",
    "        return len(parts[-1]) >= 2 and len(parts[-1]) <= 8\n",
    "\n",
    "    # --- URL VALIDATION ---\n",
    "    @staticmethod\n",
    "    def validate_url(text: str) -> bool:\n",
    "        if \"@\" in text: return False\n",
    "        if text.lower().startswith(('http:', 'https:', 'www.')): return True\n",
    "        domain_part = text.split('/')[0]\n",
    "        parts = domain_part.split('.')\n",
    "        if len(parts) < 2: return False\n",
    "        valid_tld = any(part.lower() in Validators.SAFE_TLDS for part in parts[-2:])\n",
    "        if not valid_tld: return False\n",
    "        last = parts[-1].lower()\n",
    "        return not (len(last) < 2 or last.isdigit() or last in Validators.IGNORED_FILE_EXTENSIONS)\n",
    "\n",
    "    # --- IBAN VALIDATION ---\n",
    "    @staticmethod\n",
    "    def validate_iban(text: str) -> bool:\n",
    "        def check_sum(clean_text):\n",
    "            if \"ZZZ\" in clean_text.upper(): return False\n",
    "            if len(clean_text) < 15 or len(clean_text) > 34: return False\n",
    "            try:\n",
    "                rearranged = clean_text[4:] + clean_text[:4]\n",
    "                numeric_iban = \"\"\n",
    "                for char in rearranged:\n",
    "                    if char.isalpha(): numeric_iban += str(ord(char) - 55)\n",
    "                    elif char.isdigit(): numeric_iban += char\n",
    "                    else: return False\n",
    "                return int(numeric_iban) % 97 == 1\n",
    "            except: return False\n",
    "        clean = Validators.normalize(text).upper()\n",
    "        if check_sum(clean): return True\n",
    "        fixed = Validators.fix_common_typos(clean)\n",
    "        return check_sum(fixed)\n",
    "\n",
    "    # --- CARD VALIDATION ---\n",
    "    @staticmethod\n",
    "    def validate_card(text: str) -> bool:\n",
    "        clean = Validators.normalize(text)\n",
    "        if not clean.isdigit() or not (13 <= len(clean) <= 19) or clean.startswith('0'): \n",
    "            return False\n",
    "        digits = [int(d) for d in clean]\n",
    "        checksum = 0\n",
    "        for i, digit in enumerate(reversed(digits)):\n",
    "            if i % 2 == 1:\n",
    "                doubled = digit * 2\n",
    "                checksum += doubled if doubled < 10 else doubled - 9\n",
    "            else: checksum += digit\n",
    "        return checksum % 10 == 0\n",
    "\n",
    "    @staticmethod\n",
    "    def validate_id(text: str, id_type: str) -> bool:\n",
    "        clean = Validators.normalize(text)\n",
    "        \n",
    "        # --- RUBBISH FILTER ---\n",
    "        if clean.lower() in Validators.GERMAN_RUBBISH: return False\n",
    "        if clean.isalpha() and len(clean) < 10: \n",
    "            if clean.lower() in Validators.GERMAN_RUBBISH: return False\n",
    "\n",
    "        if id_type == \"PII:ID:TAX\": return len(clean) == 11 and clean.isdigit()\n",
    "        if id_type == \"PII:ID:SVN\": return 9 <= len(clean) <= 15\n",
    "        if id_type == \"PII:ID:DRIVERLICENSE\":\n",
    "            return len(clean) == 11 and any(char.isdigit() for char in clean)\n",
    "        \n",
    "        if id_type == \"PII:ID:PASSPORT\":\n",
    "            if clean.isalpha(): return False\n",
    "            return 6 <= len(clean) <= 12\n",
    "            \n",
    "        if id_type == \"PII:ID:NATIONAL\": return 6 <= len(clean) <= 12\n",
    "        return True\n",
    "\n",
    "# ==========================================\n",
    "# 2. CONTEXT VALIDATORS (AI FALLBACK)\n",
    "# ==========================================\n",
    "class IBANContextValidator:\n",
    "    def __init__(self, threshold=0.18):\n",
    "        self.threshold = threshold\n",
    "        self.pos_anchors = [\"IBAN\", \"Konto\", \"Bankverbindung\", \"Überweisung\", \"SEPA\", \"BIC\", \"Bank\", \"Kontodaten\", \"Zahlung an\"]\n",
    "        self.neg_anchors = [\"Telefon\", \"Handy\", \"Fax\", \"ID\", \"Pass\", \"Ausweis\", \"Steuer\", \"SV-Nr\"]\n",
    "        self.vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(2, 4))\n",
    "        self.vectorizer.fit(self.pos_anchors + self.neg_anchors)\n",
    "        self.pos_vectors = self.vectorizer.transform(self.pos_anchors)\n",
    "\n",
    "    def is_valid_context(self, text: str, start: int, end: int) -> bool:\n",
    "        window = text[max(0, start-40):min(len(text), end+40)].lower()\n",
    "        input_vec = self.vectorizer.transform([window])\n",
    "        pos_score = float(np.max(cosine_similarity(input_vec, self.pos_vectors)))\n",
    "        return pos_score > self.threshold\n",
    "\n",
    "class CardContextValidator:\n",
    "    def __init__(self, threshold=0.25):\n",
    "        self.threshold = threshold\n",
    "        self.pos_anchors = [\"Kreditkarte\", \"Mastercard\", \"Visa\", \"Amex\", \"American Express\", \"Karteninhaber\", \"Gültigkeit\", \"endend auf\", \"ending in\", \"Ablaufdatum\", \"Zahlung\", \"Karte\", \"Credit Card\", \"Girocard\", \"EC-Karte\"]\n",
    "        self.neg_anchors = [\"Geburtstag\", \"Telefon\", \"Hausnummer\", \"PLZ\", \"Postleitzahl\", \"Jahr\", \"Uhrzeit\", \"Euro\", \"EUR\", \"PIN\", \"CVV\", \"CVC\", \"Prüfziffer\", \"Code\", \"TAN\", \"IBAN\", \"Bic\", \"Tel\", \"Fax\"]\n",
    "        self.vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(2, 4))\n",
    "        self.vectorizer.fit(self.pos_anchors + self.neg_anchors)\n",
    "        self.pos_vectors = self.vectorizer.transform(self.pos_anchors)\n",
    "        self.neg_vectors = self.vectorizer.transform(self.neg_anchors)\n",
    "\n",
    "    def is_valid_context(self, text: str, start: int, end: int) -> bool:\n",
    "        window = text[max(0, start-60):min(len(text), end+60)].lower()\n",
    "        input_vec = self.vectorizer.transform([window])\n",
    "        pos_score = float(np.max(cosine_similarity(input_vec, self.pos_vectors)))\n",
    "        neg_score = float(np.max(cosine_similarity(input_vec, self.neg_vectors)))\n",
    "        return pos_score > self.threshold and pos_score > neg_score\n",
    "\n",
    "class PhoneContextValidator:\n",
    "    def __init__(self, threshold=0.08):\n",
    "        self.threshold = threshold\n",
    "        self.pos_anchors = [\"tel\", \"telefon\", \"phone\", \"mobil\", \"handy\", \"fon\", \"nummer\", \"nr\", \"rückruf\", \"contact\", \"kontakt\", \"anrufen\", \"angerufen\", \"durchwahl\", \"mobile\", \"erreichbar\", \"unter\", \"ansprechpartner\"]\n",
    "        self.neg_anchors = [\"laufzeit\", \"zeitraum\", \"datum\", \"iban\", \"bic\", \"steuer\", \"id\", \"konto\", \"bank\", \"betrag\", \"euro\", \"eur\", \"plz\", \"gesamtsumme\"]\n",
    "        self.vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(2, 4))\n",
    "        self.vectorizer.fit(self.pos_anchors + self.neg_anchors)\n",
    "        self.pos_vectors = self.vectorizer.transform(self.pos_anchors)\n",
    "        self.neg_vectors = self.vectorizer.transform(self.neg_anchors)\n",
    "\n",
    "    def is_valid_context(self, text: str, start: int, end: int) -> bool:\n",
    "        window = text[max(0, start-60):min(len(text), end+60)].lower()\n",
    "        if any(kw in window for kw in [\"tel\", \"mobil\", \"handy\", \"fon\", \"nummer\", \"nr.\"]): return True\n",
    "        clean_num = re.sub(r\"[^0-9]\", \"\", text[start:end])\n",
    "        if clean_num.startswith(\"01\") and 10 <= len(clean_num) <= 13: return True\n",
    "            \n",
    "        input_vec = self.vectorizer.transform([window])\n",
    "        pos_score = float(np.max(cosine_similarity(input_vec, self.pos_vectors)))\n",
    "        neg_score = float(np.max(cosine_similarity(input_vec, self.neg_vectors)))\n",
    "        return pos_score > self.threshold and (pos_score >= neg_score or pos_score > 0.20)\n",
    "\n",
    "class PassportContextValidator:\n",
    "    def __init__(self, threshold=0.18):\n",
    "        self.threshold = threshold\n",
    "        self.pos_anchors = [\"Pass\", \"Reisepass\", \"Passport\", \"Passnummer\", \"Pass-Nr\", \"Pass No\", \"Visa\", \"Nationalität\", \"Dokument\"]\n",
    "        self.neg_anchors = [\n",
    "            \"IBAN\", \"BIC\", \"Konto\", \"Bank\", \"Euro\", \"Zahlung\", \"Lastschrift\",\n",
    "            \"Telefon\", \"Handy\", \"Tel\", \"Mobil\", \"Nummer\", \"Rückruf\",\n",
    "            \"Jahre\", \"alt\", \"geboren\", \"geb.\", \"Geburtsdatum\",\n",
    "            \"Führerschein\", \"License\", \"FS-Nr\", \"Klasse\", \"Fahrer\",\n",
    "            \"Versicherung\", \"SV-Nr\", \"Krankenkasse\", \"Steuer-ID\"\n",
    "        ]\n",
    "        self.vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(2, 4))\n",
    "        self.vectorizer.fit(self.pos_anchors + self.neg_anchors)\n",
    "        self.pos_vectors = self.vectorizer.transform(self.pos_anchors)\n",
    "        self.neg_vectors = self.vectorizer.transform(self.neg_anchors)\n",
    "\n",
    "    def is_valid_context(self, text: str, start: int, end: int, candidate: str) -> bool:\n",
    "        pre_window = text[max(0, start-25):start].lower()\n",
    "        if \"pass\" in pre_window or \"reise\" in pre_window: return True\n",
    "        window = text[max(0, start-60):min(len(text), end+60)].lower()\n",
    "        input_vec = self.vectorizer.transform([window])\n",
    "        pos_score = float(np.max(cosine_similarity(input_vec, self.pos_vectors)))\n",
    "        neg_score = float(np.max(cosine_similarity(input_vec, self.neg_vectors)))\n",
    "        if candidate.isdigit(): return pos_score > (self.threshold * 1.5) and pos_score > neg_score\n",
    "        return pos_score > self.threshold and pos_score > neg_score\n",
    "\n",
    "class SVNContextValidator:\n",
    "    def __init__(self, threshold=0.15):\n",
    "        self.threshold = threshold\n",
    "        self.pos_anchors = [\"SV-Nummer\", \"SV-Nr\", \"Sozialversicherung\", \"Rentenversicherung\", \"Versicherungsnummer\", \"RV-NR\", \"SVNR\"]\n",
    "        self.neg_anchors = [\"IBAN\", \"Konto\", \"Pass\", \"Telefon\", \"Handy\", \"Steuer\", \"Tax\"]\n",
    "        self.vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(2, 4))\n",
    "        self.vectorizer.fit(self.pos_anchors + self.neg_anchors)\n",
    "        self.pos_vectors = self.vectorizer.transform(self.pos_anchors)\n",
    "\n",
    "    def is_valid_context(self, text: str, start: int, end: int) -> bool:\n",
    "        window = text[max(0, start-50):min(len(text), end+50)].lower()\n",
    "        if any(kw in window for kw in [\"sv-nr\", \"svnr\", \"rv-nr\", \"sozialversicherung\"]): return True\n",
    "        input_vec = self.vectorizer.transform([window])\n",
    "        pos_score = float(np.max(cosine_similarity(input_vec, self.pos_vectors)))\n",
    "        return pos_score > self.threshold\n",
    "\n",
    "class NationalContextValidator:\n",
    "    def __init__(self, threshold=0.18):\n",
    "        self.threshold = threshold\n",
    "        self.pos_anchors = [\"Ausweis\", \"Personalausweis\", \"National ID\", \"ID-Nr\", \"Identitätskarte\", \"Ausweisnummer\", \"Perso\", \"Dokumentnummer\"]\n",
    "        # Explicit Negative Anchors to separate from Driver's License\n",
    "        self.neg_anchors = [\"Führerschein\", \"License\", \"Driver\", \"Klasse\", \"Fahrerlaubnis\", \"Fahrzeug\", \"IBAN\", \"Konto\", \"Telefon\", \"SV-Nr\"]\n",
    "        \n",
    "        self.vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(2, 4))\n",
    "        self.vectorizer.fit(self.pos_anchors + self.neg_anchors)\n",
    "        self.pos_vectors = self.vectorizer.transform(self.pos_anchors)\n",
    "        self.neg_vectors = self.vectorizer.transform(self.neg_anchors)\n",
    "\n",
    "    def is_valid_context(self, text: str, start: int, end: int, candidate: str) -> bool:\n",
    "        pre_window = text[max(0, start-25):start].lower()\n",
    "        if any(kw in pre_window for kw in [\"ausweis\", \"perso\", \"id-nr\"]): return True\n",
    "        \n",
    "        window = text[max(0, start-60):min(len(text), end+60)].lower()\n",
    "        input_vec = self.vectorizer.transform([window])\n",
    "        pos_score = float(np.max(cosine_similarity(input_vec, self.pos_vectors)))\n",
    "        neg_score = float(np.max(cosine_similarity(input_vec, self.neg_vectors)))\n",
    "        \n",
    "        # Numeric strings require a higher \"National ID\" signature to avoid false positives\n",
    "        if candidate.isdigit():\n",
    "            return pos_score > (self.threshold * 1.5) and pos_score > neg_score\n",
    "        return pos_score > self.threshold and pos_score > neg_score\n",
    "\n",
    "# ==========================================\n",
    "# 3. USER PROVIDED DETECTOR\n",
    "# ==========================================\n",
    "class RegexPIIDetector:\n",
    "    def __init__(self, iban_validator=None, card_validator=None, phone_validator=None, passport_validator=None, svn_validator=None, national_validator=None):\n",
    "        self.iban_validator = iban_validator\n",
    "        self.card_validator = card_validator\n",
    "        self.phone_validator = phone_validator\n",
    "        self.passport_validator = passport_validator\n",
    "        self.svn_validator = svn_validator\n",
    "        self.national_validator = national_validator\n",
    "        \n",
    "        self.patterns = {\n",
    "            \"FINANCIAL:IBAN\": re.compile(r\"\\b[A-Z]{2}\\d{2}(?:[\\s\\.\\-]*[A-Z0-9]){11,35}\\b\", re.IGNORECASE),\n",
    "            \"FINANCIAL:CARD\": re.compile(r\"\\b[1-9](?:[\\s\\-\\–\\/\\.]*\\d){12,18}\\b\"),\n",
    "            \"FINANCIAL:CARD_PARTIAL_INTERNAL\": re.compile(r\"(?i)(?:visa|mastercard|amex|girocard|kreditkarte|karte|endend|ending)\\s*(?:auf|in|no|nr)?\\s*(?::|#)?\\s*\\b(\\d{4})\\b\"),\n",
    "            \"CONTACT:URL\": re.compile(r\"\\b(?:(?:https?://|www\\.)[\\w\\-\\.\\/%\\+~=\\?&]+|[\\w\\-]+(?:\\.[a-zA-Z]{2,})+(?:/[\\w\\-\\.\\/%\\+~=\\?&]*)?)\\b\", re.IGNORECASE),\n",
    "            \"CONTACT:PHONE\": re.compile(r\"(?i)(?<![a-zA-Z0-9\\.])(?:(?:(?:\\+|00|[o0])[1-9]\\d{0,4})[\\s\\.\\-\\/\\\\]*(?:\\(\\s*[o0]\\s*\\)\\s*)?|(?:\\(\\s*[o0][1-9]\\d{1,8}\\s*\\)|[o0][1-9]\\d{1,8}))(?:[ \\t\\.\\-\\/\\\\]*(?:onal|abortel)?[ \\t\\.\\-\\/\\\\]*\\d){3,15}[l1]?(?![a-zA-Z0-9])\"),\n",
    "            \"CONTACT:EMAIL\": re.compile(r\"\\b[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,6}(?=[^a-z]|$)\"),\n",
    "            \"PII:ID:TAX\": re.compile(r\"\\b[1-9]\\d{10}\\b\"),\n",
    "            \"PII:ID:SVN\": re.compile(r\"\\b\\d{2}[\\s]*\\d{6}[\\s]*[A-Z][\\s]*\\d{3}\\b\", re.IGNORECASE),\n",
    "            \"PII:ID:DRIVERLICENSE\": re.compile(r\"\\b(?=.*\\d)(?=.*[A-Z])[A-Z0-9]{11}\\b\", re.IGNORECASE),\n",
    "            \"PII:ID:PASSPORT\": re.compile(r\"\\b[A-Z0-9]{7,11}\\b\", re.IGNORECASE),\n",
    "            \"PII:ID:NATIONAL\": re.compile(r\"\\b[L-Z0-9][0-9A-Z]{8,9}\\b\"),\n",
    "        }\n",
    "\n",
    "    def _validate_match(self, pii_type: str, text: str) -> bool:\n",
    "        if pii_type.startswith(\"PII:ID\"): return Validators.validate_id(text, pii_type)\n",
    "        if pii_type == \"CONTACT:URL\": return Validators.validate_url(text)\n",
    "        elif pii_type == \"CONTACT:PHONE\": return Validators.validate_phone(text)\n",
    "        elif pii_type == \"CONTACT:EMAIL\": return Validators.validate_email(text)\n",
    "        elif pii_type == \"FINANCIAL:IBAN\": return Validators.validate_iban(text)\n",
    "        elif pii_type == \"FINANCIAL:CARD\": return Validators.validate_card(text)\n",
    "        return True\n",
    "\n",
    "    def _resolve_conflicts(self, detections: List[Dict]) -> List[Dict]:\n",
    "        if not detections: return []\n",
    "        priority = {\n",
    "            \"FINANCIAL:IBAN\": 100, \"FINANCIAL:CARD\": 95, \"CONTACT:EMAIL\": 92, \n",
    "            \"PII:ID:SVN\": 90, \"PII:ID:DRIVERLICENSE\": 85, \"PII:ID:PASSPORT\": 85, \n",
    "            \"PII:ID:NATIONAL\": 85, \"PII:ID:TAX\": 75, \"CONTACT:URL\": 50, \"CONTACT:PHONE\": 10\n",
    "        }\n",
    "        sorted_dets = sorted(detections, key=lambda x: (-priority.get(x['type'], 0), -(x['end'] - x['start'])))\n",
    "        final, occupied = [], set()\n",
    "        for det in sorted_dets:\n",
    "            r = range(det['start'], det['end'])\n",
    "            if not any(i in occupied for i in r):\n",
    "                final.append(det); occupied.update(r)\n",
    "        return sorted(final, key=lambda x: x['start'])\n",
    "\n",
    "    def detect(self, text: str) -> List[Dict]:\n",
    "        raw_detections = []\n",
    "        for pii_type, pattern in self.patterns.items():\n",
    "            for match in pattern.finditer(text):\n",
    "                is_valid = False\n",
    "                start_pos, end_pos = match.start(), match.end()\n",
    "                detection_type = pii_type\n",
    "                \n",
    "                if pii_type == \"FINANCIAL:CARD_PARTIAL_INTERNAL\":\n",
    "                    detection_type = \"FINANCIAL:CARD\"\n",
    "                    original_digits = match.group(1)\n",
    "                    clean_text = f\"****{original_digits}\" \n",
    "                    start_pos, end_pos = match.start(1), match.end(1)\n",
    "                    if self.card_validator and self.card_validator.is_valid_context(text, start_pos, end_pos):\n",
    "                        is_valid = True\n",
    "                \n",
    "                elif pii_type == \"FINANCIAL:IBAN\":\n",
    "                    candidate = match.group()\n",
    "                    parts = re.split(r'(\\s+)', candidate)\n",
    "                    stop_labels = {\"BIC\", \"STOP\", \"ICH\", \"NAME\", \"GMBH\", \"IBAN\", \"AN\", \"AUF\", \"AM\", \"BIN\", \"WAR\", \"VON\", \"UND\", \"DIE\", \"IST\", \"DER\", \"DAS\", \"DAME\", \"HERR\", \"WIR\", \"IHR\", \"SEIN\", \"MIT\", \"BEI\"}\n",
    "                    valid_parts = []\n",
    "                    word_idx = 0\n",
    "                    for p in parts:\n",
    "                        if not p.strip(): valid_parts.append(p); continue\n",
    "                        t = p.strip(\".,;:!?() \")\n",
    "                        if not t: continue\n",
    "                        if t.upper() in stop_labels or \"ZZZ\" in t.upper(): break\n",
    "                        if t[0].isupper() and any(c.islower() for c in t): break\n",
    "                        if word_idx > 0 and not any(c.isdigit() for c in t) and len(t) != 4: break\n",
    "                        valid_parts.append(p); word_idx += 1\n",
    "                    clean_text = \"\".join(valid_parts).strip(\".,;:!? \")\n",
    "                    end_pos = match.start() + len(clean_text)\n",
    "                    if Validators.validate_iban(clean_text): is_valid = True\n",
    "                    elif self.iban_validator and self.iban_validator.is_valid_context(text, match.start(), end_pos):\n",
    "                        if 15 <= len(Validators.normalize(clean_text)) <= 34: is_valid = True\n",
    "                \n",
    "                elif pii_type == \"FINANCIAL:CARD\":\n",
    "                    if match.start() > 0 and text[match.start()-1] == '+': continue\n",
    "                    clean_text = match.group().strip(\".,;:!? \")\n",
    "                    end_pos = match.start() + len(clean_text)\n",
    "                    if Validators.validate_card(clean_text): is_valid = True\n",
    "                    elif self.card_validator and self.card_validator.is_valid_context(text, match.start(), end_pos): is_valid = True\n",
    "                \n",
    "                elif pii_type == \"CONTACT:PHONE\":\n",
    "                    clean_text = match.group().strip(\".,;:!? \\n\\r\\t\")\n",
    "                    end_pos = match.start() + len(clean_text)\n",
    "                    if Validators.validate_phone(clean_text):\n",
    "                        if self.phone_validator:\n",
    "                            if self.phone_validator.is_valid_context(text, match.start(), end_pos):\n",
    "                                is_valid = True\n",
    "                        else: is_valid = True\n",
    "\n",
    "                elif pii_type == \"PII:ID:PASSPORT\":\n",
    "                    clean_text = match.group().strip(\".,;:!? \\n\\r\\t\")\n",
    "                    end_pos = match.start() + len(clean_text)\n",
    "                    if Validators.validate_id(clean_text, pii_type):\n",
    "                        if self.passport_validator and self.passport_validator.is_valid_context(text, start_pos, end_pos, clean_text):\n",
    "                            is_valid = True\n",
    "\n",
    "                elif pii_type == \"PII:ID:SVN\":\n",
    "                    clean_text = match.group().strip(\".,;:!? \\n\\r\\t\")\n",
    "                    end_pos = match.start() + len(clean_text)\n",
    "                    if Validators.validate_id(clean_text, pii_type):\n",
    "                        if self.svn_validator:\n",
    "                            if self.svn_validator.is_valid_context(text, match.start(), end_pos):\n",
    "                                is_valid = True\n",
    "                        else: is_valid = True\n",
    "\n",
    "                elif pii_type == \"PII:ID:NATIONAL\":\n",
    "                    clean_text = match.group().strip(\".,;:!? \\n\\r\\t\")\n",
    "                    end_pos = match.start() + len(clean_text)\n",
    "                    if Validators.validate_id(clean_text, pii_type):\n",
    "                        if self.national_validator:\n",
    "                            if self.national_validator.is_valid_context(text, match.start(), end_pos, clean_text):\n",
    "                                is_valid = True\n",
    "                        else: is_valid = True\n",
    "\n",
    "                else:\n",
    "                    clean_text = match.group().strip(\".,;:!? \")\n",
    "                    end_pos = match.start() + len(clean_text)\n",
    "                    is_valid = self._validate_match(pii_type, clean_text)\n",
    "\n",
    "                if is_valid:\n",
    "                    raw_detections.append({\"type\": detection_type, \"text\": clean_text, \"start\": start_pos, \"end\": end_pos, \"confidence\": 1.0})\n",
    "        return self._resolve_conflicts(raw_detections)\n",
    "\n",
    "# ==========================================\n",
    "# 4. AGE EXTRACTION (CONTEXTUAL)\n",
    "# ==========================================\n",
    "class FastAgeExtractor:\n",
    "    def __init__(self, threshold=0.30):\n",
    "        self.threshold = threshold\n",
    "        self.current_year = datetime.now().year\n",
    "        self.pos_anchors = [\"Ich bin <NUM> Jahre alt\", \"Er ist <NUM> geworden\", \"Sie ist <NUM>\", \"Mein Alter ist <NUM>\", \"Das Kind ist <NUM>\", \"Ein <NUM>-Jähriger\", \"Mit <NUM> Jahren\", \"Geboren am <NUM>\", \"Mein Geburtsdatum ist <NUM>\", \"Baujahr <NUM>\", \"Jahrgang <NUM>\", \"Geburtstag am <NUM>\", \"Nächstes Jahr werde ich <NUM>\", \"Der ist schon <NUM>\"]\n",
    "        self.neg_anchors = [\"Das kostet <NUM> Euro\", \"Preis <NUM> EUR\", \"Ich habe <NUM> Äpfel\", \"In <NUM> Minuten\", \"Hausnummer <NUM>\", \"Seite <NUM>\", \"Um <NUM> Uhr\", \"<NUM> Prozent\", \"Gewicht <NUM> kg\", \"PLZ <NUM>\", \"Verspätung <NUM>\", \"Nummer <NUM>\", \"Platz <NUM>\", \"Größe <NUM>\"]\n",
    "        self.vectorizer = TfidfVectorizer(analyzer='word', ngram_range=(1, 2))\n",
    "        self.vectorizer.fit(self.pos_anchors + self.neg_anchors)\n",
    "        self.pos_vectors = self.vectorizer.transform(self.pos_anchors)\n",
    "        self.neg_vectors = self.vectorizer.transform(self.neg_anchors)\n",
    "\n",
    "    def calculate_age(self, value_str):\n",
    "        if re.search(r'[./-]', value_str):\n",
    "            for fmt in (\"%d.%m.%Y\", \"%d/%m/%Y\", \"%d-%m-%Y\"):\n",
    "                try:\n",
    "                    dob = datetime.strptime(value_str, fmt)\n",
    "                    age = self.current_year - dob.year - ((datetime.now().month, datetime.now().day) < (dob.month, dob.day))\n",
    "                    return age\n",
    "                except ValueError: continue\n",
    "            return None\n",
    "        try:\n",
    "            val = int(value_str)\n",
    "            if 1900 < val <= self.current_year: return self.current_year - val\n",
    "            return val\n",
    "        except ValueError: return None\n",
    "\n",
    "    def get_pii_type(self, age):\n",
    "        if age is None: return \"AGE:UNKNOWN\"\n",
    "        if age > 120: return None\n",
    "        if age < 12: return \"AGE:CHILD\"\n",
    "        elif 12 <= age <= 17: return \"AGE:TEEN\"\n",
    "        elif 18 <= age <= 64: return \"AGE:ADULT\"\n",
    "        else: return \"AGE:SENIOR\"\n",
    "\n",
    "    def analyze_text(self, text):\n",
    "        findings = []\n",
    "        for match in re.finditer(r'\\b(\\d{1,2}[./-]\\d{1,2}[./-]\\d{4}|\\d{1,4})\\b', text):\n",
    "            cand = match.group(0)\n",
    "            snip = text[max(0, match.start()-50):min(len(text), match.end()+50)].replace(cand, \"<NUM>\", 1)\n",
    "            vec = self.vectorizer.transform([snip])\n",
    "            p_scr = float(np.max(cosine_similarity(vec, self.pos_vectors)))\n",
    "            n_scr = float(np.max(cosine_similarity(vec, self.neg_vectors)))\n",
    "            if p_scr > self.threshold and p_scr > n_scr:\n",
    "                age = self.calculate_age(cand)\n",
    "                label = self.get_pii_type(age)\n",
    "                if label: findings.append({\"type\": label, \"text\": cand, \"start\": match.start(), \"end\": match.end(), \"confidence\": round(p_scr, 2)})\n",
    "        return findings\n",
    "\n",
    "# ==========================================\n",
    "# 5. UNIFIED PIPELINE (MERGED LOGIC)\n",
    "# ==========================================\n",
    "class UnifiedPIIPipeline:\n",
    "    def __init__(self):\n",
    "        print(\"Loading NLP Models...\")\n",
    "        try: self.nlp = spacy.load(\"de_core_news_lg\")\n",
    "        except:\n",
    "            from spacy.cli import download\n",
    "            download(\"de_core_news_lg\")\n",
    "            self.nlp = spacy.load(\"de_core_news_lg\")\n",
    "\n",
    "        self.medications = [\"ibuprofen\", \"aspirin\", \"paracetamol\", \"antibiotika\", \"insulin\"]\n",
    "        self.conditions = [\"kopfschmerzen\", \"migräne\", \"fieber\", \"husten\", \"diabetes\"]\n",
    "        self.stoplist = {\"Morgen\", \"Heute\", \"Gestern\", \"Hallo\", \"Hi\", \"Hey\", \"Danke\", \"Bitte\", \"Grüße\"}\n",
    "        self.noun_blocklist = {\"Perso\", \"Ausweis\", \"Pass\", \"Konto\", \"Bank\", \"Iban\", \"Nummer\", \"Tel\", \"Handy\", \"Telefon\", \"Email\", \"Mail\", \"Adresse\", \"Name\", \"Büro\", \"Handynummer\", \"Steuer-ID\", \"Glückszahl\", \"SV-Nr\", \"SVN\"}\n",
    "\n",
    "        ruler = self.nlp.add_pipe(\"entity_ruler\", before=\"ner\")\n",
    "        patterns = []\n",
    "        for item in self.medications: patterns.append({\"label\": \"MEDICATION\", \"pattern\": [{\"LOWER\": item}]})\n",
    "        for item in self.conditions: patterns.append({\"label\": \"CONDITION\", \"pattern\": [{\"LOWER\": item}]})\n",
    "        sfx = \"straße|strasse|str.|weg|platz|allee|damm|ring|gasse|ufer|chaussee|hof|garten|markt|zeile|wall\"\n",
    "        patterns.append({\"label\": \"ADDRESS_DETECTED\", \"pattern\": [{\"TEXT\": {\"REGEX\": f\"(?i).+({sfx})$\"}}, {\"TEXT\": {\"REGEX\": r\"^\\d\"}}]})\n",
    "        ruler.add_patterns(patterns)\n",
    "\n",
    "        self.matcher = DependencyMatcher(self.nlp.vocab)\n",
    "        self.age_extractor = FastAgeExtractor(threshold=0.30)\n",
    "        self.iban_context_validator = IBANContextValidator()\n",
    "        self.card_context_validator = CardContextValidator()\n",
    "        self.phone_context_validator = PhoneContextValidator()\n",
    "        self.passport_context_validator = PassportContextValidator()\n",
    "        self.svn_context_validator = SVNContextValidator()\n",
    "        self.national_validator = NationalContextValidator()\n",
    "        \n",
    "        self.regex_detector = RegexPIIDetector(\n",
    "            iban_validator=self.iban_context_validator, \n",
    "            card_validator=self.card_context_validator,\n",
    "            phone_validator=self.phone_context_validator,\n",
    "            passport_validator=self.passport_context_validator,\n",
    "            svn_validator=self.svn_context_validator,\n",
    "            national_validator=self.national_validator\n",
    "        )\n",
    "\n",
    "    def _generate_token(self, pii_type, text_segment):\n",
    "        h = hashlib.md5(text_segment.lower().encode()).hexdigest()[:8]\n",
    "        return f\"[PII:{pii_type.replace(':', '_')}_ID_{h}]\"\n",
    "\n",
    "    def _analyze_person_entity(self, ent):\n",
    "        role_keywords = {\"Dr.\", \"Prof.\", \"Arzt\", \"Ärztin\", \"Herr\", \"Frau\", \"Anwalt\"}\n",
    "        detected_role = \"N/A\"\n",
    "        clean_name_parts = []\n",
    "        for token in ent:\n",
    "            if token.text in role_keywords: detected_role = token.text\n",
    "            else: clean_name_parts.append(token.text)\n",
    "        if detected_role == \"N/A\" and ent.start > 0:\n",
    "            prev_token = ent.doc[ent.start - 1]\n",
    "            if prev_token.text in role_keywords: detected_role = prev_token.text\n",
    "        clean_name = re.sub(r\"[^\\w\\s-]\", \"\", \" \".join(clean_name_parts).strip())\n",
    "        return clean_name or ent.text, detected_role\n",
    "\n",
    "    def process_batch(self, text_list: List[str]) -> List[Dict[str, Any]]:\n",
    "        results = []\n",
    "        for original_text in text_list:\n",
    "            start_time = time.time()\n",
    "            all_findings = []\n",
    "            occupied = set()\n",
    "\n",
    "            regex_findings = self.regex_detector.detect(original_text)\n",
    "            all_findings.extend(regex_findings)\n",
    "            for f in regex_findings: occupied.update(range(f['start'], f['end']))\n",
    "\n",
    "            for f in self.age_extractor.analyze_text(original_text):\n",
    "                if not any(i in occupied for i in range(f['start'], f['end'])):\n",
    "                    all_findings.append(f); occupied.update(range(f['start'], f['end']))\n",
    "\n",
    "            doc = self.nlp(original_text)\n",
    "            for ent in doc.ents:\n",
    "                if any(i in occupied for i in range(ent.start_char, ent.end_char)): continue\n",
    "                label, txt = ent.label_, ent.text\n",
    "                if label in [\"MEDICATION\", \"CONDITION\"]:\n",
    "                    all_findings.append({\"type\": f\"MED:{label}\", \"text\": txt, \"start\": ent.start_char, \"end\": ent.end_char, \"confidence\": 0.9})\n",
    "                    occupied.update(range(ent.start_char, ent.end_char))\n",
    "                elif label in [\"LOC\", \"GPE\", \"ADDRESS_DETECTED\"]:\n",
    "                    if label == \"ADDRESS_DETECTED\" or re.search(r\"\\d\", txt):\n",
    "                        all_findings.append({\"type\": \"LOCATION:ADDRESS\", \"text\": txt, \"start\": ent.start_char, \"end\": ent.end_char, \"confidence\": 0.95})\n",
    "                        occupied.update(range(ent.start_char, ent.end_char))\n",
    "                elif label in [\"PER\", \"PER_STRONG\"]:\n",
    "                    clean_name, role = self._analyze_person_entity(ent)\n",
    "                    if clean_name.title() not in self.stoplist and not any(p.strip().title() in self.noun_blocklist for p in clean_name.split()):\n",
    "                        all_findings.append({\"type\": \"PERSON\", \"text\": txt, \"start\": ent.start_char, \"end\": ent.end_char, \"confidence\": 0.9})\n",
    "                        occupied.update(range(ent.start_char, ent.end_char))\n",
    "\n",
    "            all_findings.sort(key=lambda x: x['start'], reverse=True)\n",
    "            masked_text = original_text\n",
    "            for f in all_findings:\n",
    "                token = self._generate_token(f['type'], f['text'])\n",
    "                masked_text = masked_text[:f['start']] + token + masked_text[f['end']:]\n",
    "                f['token'] = token\n",
    "\n",
    "            results.append({\"has_pii\": len(all_findings) > 0, \"detections\": all_findings[::-1], \"anonymized_text\": masked_text, \"processing_time_ms\": int((time.time() - start_time) * 1000)})\n",
    "        return results\n",
    "\n",
    "# ==========================================\n",
    "# 6. EXECUTION\n",
    "# ==========================================\n",
    "if __name__ == \"__main__\":\n",
    "    pipeline = UnifiedPIIPipeline()\n",
    "    samples = [\n",
    "        \"Personalausweis Nummer: L12345678\",\n",
    "        \"Reisepass Ukraine: FE893746\",\n",
    "        \"Meine SV-Nummer lautet 11240601S003\",\n",
    "        \"Ansprechpartner: Herr Brinkmann 0228 9182736\"\n",
    "    ]\n",
    "    output = pipeline.process_batch(samples)\n",
    "    print(json.dumps(output, indent=4, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fec18de5-191f-4d07-ab4f-8233b973f452",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "newdf = pd.read_excel('PII_training_v2.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "746185d6-4b41-4ad8-a40c-0a977a32c2a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "      <th>PII:AGE:CHILD</th>\n",
       "      <th>PII:AGE:TEEN</th>\n",
       "      <th>PII:AGE:ADULT</th>\n",
       "      <th>PII:AGE:SENIOR</th>\n",
       "      <th>PII:PERSON</th>\n",
       "      <th>PII:CONTACT:EMAIL</th>\n",
       "      <th>PII:CONTACT:PHONE</th>\n",
       "      <th>PII:FINANCIAL:IBAN</th>\n",
       "      <th>PII:FINANCIAL:BIC</th>\n",
       "      <th>...</th>\n",
       "      <th>PII:ID:PASSPORT</th>\n",
       "      <th>PII:ID:TAX</th>\n",
       "      <th>PII:ID:UST</th>\n",
       "      <th>PII:ID:SVN</th>\n",
       "      <th>PII:ID:INSURANCE</th>\n",
       "      <th>PII:ID:DRIVERLICENSE</th>\n",
       "      <th>PII:DRIVERPLATE</th>\n",
       "      <th>PII:LOCATION:POSTALCODE</th>\n",
       "      <th>PII:BIRTHDAY</th>\n",
       "      <th>PII:LOCATION:ADDRESS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kannst du mir helfen diese Kundenanfrage zu be...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Thorsten Beyer</td>\n",
       "      <td>Noch</td>\n",
       "      <td>0211 7839456</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40215</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Marktstraße 23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hey, schreib mir mal ne kurze zahlungserinneru...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sabine Kröger</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>45147</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hufelandring 8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Folgene Email ist reingekommen, kannst du mir ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Monika Schuster-Brandt</td>\n",
       "      <td>m.schuster-brandt@gmx.de</td>\n",
       "      <td>0221-4478832</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50931</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lessingallee 45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>muss dem neuen mitarbeiter ne willkommensmail ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Aleksandar Petrović</td>\n",
       "      <td>a.petrovic@web.de</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.08.1992</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hab hier nen Zettel vom Außendienst, kann kaum...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Winkelmann; Winkelman</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000124</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28215</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Theodor-Heuss-Allee 78</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           sentences PII:AGE:CHILD  \\\n",
       "0  Kannst du mir helfen diese Kundenanfrage zu be...           NaN   \n",
       "1  hey, schreib mir mal ne kurze zahlungserinneru...           NaN   \n",
       "2  Folgene Email ist reingekommen, kannst du mir ...           NaN   \n",
       "3  muss dem neuen mitarbeiter ne willkommensmail ...           NaN   \n",
       "4  Hab hier nen Zettel vom Außendienst, kann kaum...           NaN   \n",
       "\n",
       "  PII:AGE:TEEN PII:AGE:ADULT PII:AGE:SENIOR              PII:PERSON  \\\n",
       "0          NaN           NaN            NaN          Thorsten Beyer   \n",
       "1          NaN           NaN            NaN           Sabine Kröger   \n",
       "2          NaN           NaN            NaN  Monika Schuster-Brandt   \n",
       "3          NaN           NaN            NaN     Aleksandar Petrović   \n",
       "4          NaN           NaN            NaN   Winkelmann; Winkelman   \n",
       "\n",
       "          PII:CONTACT:EMAIL PII:CONTACT:PHONE PII:FINANCIAL:IBAN  \\\n",
       "0                     Noch       0211 7839456                NaN   \n",
       "1                       NaN               NaN                NaN   \n",
       "2  m.schuster-brandt@gmx.de      0221-4478832                NaN   \n",
       "3         a.petrovic@web.de               NaN                NaN   \n",
       "4                       NaN          0.000124                NaN   \n",
       "\n",
       "  PII:FINANCIAL:BIC  ... PII:ID:PASSPORT  PII:ID:TAX PII:ID:UST PII:ID:SVN  \\\n",
       "0               NaN  ...             NaN         NaN        NaN        NaN   \n",
       "1               NaN  ...             NaN         NaN        NaN        NaN   \n",
       "2               NaN  ...             NaN         NaN        NaN        NaN   \n",
       "3               NaN  ...             NaN         NaN        NaN        NaN   \n",
       "4               NaN  ...             NaN         NaN        NaN        NaN   \n",
       "\n",
       "  PII:ID:INSURANCE PII:ID:DRIVERLICENSE PII:DRIVERPLATE  \\\n",
       "0              NaN                  NaN             NaN   \n",
       "1              NaN                  NaN             NaN   \n",
       "2              NaN                  NaN             NaN   \n",
       "3              NaN                  NaN             NaN   \n",
       "4              NaN                  NaN             NaN   \n",
       "\n",
       "  PII:LOCATION:POSTALCODE PII:BIRTHDAY    PII:LOCATION:ADDRESS  \n",
       "0                   40215          NaN          Marktstraße 23  \n",
       "1                   45147          NaN          Hufelandring 8  \n",
       "2                   50931          NaN         Lessingallee 45  \n",
       "3                     NaN   14.08.1992                     NaN  \n",
       "4                   28215          NaN  Theodor-Heuss-Allee 78  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5be7ec6e-874a-4dfe-ba2e-d06f221ad0c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['sentences', 'PII:AGE:CHILD', 'PII:AGE:TEEN', 'PII:AGE:ADULT',\n",
       "       'PII:AGE:SENIOR', 'PII:PERSON', 'PII:CONTACT:EMAIL',\n",
       "       'PII:CONTACT:PHONE', 'PII:FINANCIAL:IBAN', 'PII:FINANCIAL:BIC',\n",
       "       'PII:FINANCIAL:CARD', 'PII:PIN', 'PII:ID:NATIONAL', 'PII:ID:PASSPORT',\n",
       "       'PII:ID:TAX', 'PII:ID:UST', 'PII:ID:SVN', 'PII:ID:INSURANCE',\n",
       "       'PII:ID:DRIVERLICENSE', 'PII:DRIVERPLATE', 'PII:LOCATION:POSTALCODE',\n",
       "       'PII:BIRTHDAY', 'PII:LOCATION:ADDRESS'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39640606-bb73-4eee-bd9f-c0b1cc5b11e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df = newdf[['sentences', 'PII:ID:NATIONAL']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f3ae90f-60ff-47a0-80b9-d22dabaeb80a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading NLP Models...\n",
      "                                           sentences PII:ID:NATIONAL  \\\n",
      "0  Kannst du mir helfen diese Kundenanfrage zu be...             NaN   \n",
      "1  hey, schreib mir mal ne kurze zahlungserinneru...             NaN   \n",
      "2  Folgene Email ist reingekommen, kannst du mir ...             NaN   \n",
      "3  muss dem neuen mitarbeiter ne willkommensmail ...             NaN   \n",
      "4  Hab hier nen Zettel vom Außendienst, kann kaum...             NaN   \n",
      "\n",
      "  detected_card  \n",
      "0          None  \n",
      "1          None  \n",
      "2          None  \n",
      "3          None  \n",
      "4          None  \n"
     ]
    }
   ],
   "source": [
    "pipeline = UnifiedPIIPipeline()\n",
    "samples = processed_df['sentences'].astype(str).tolist()\n",
    "output = pipeline.process_batch(samples)\n",
    "\n",
    "extracted_ibans = []\n",
    "\n",
    "for result in output:\n",
    "    ibans_in_sentence = [d['text'] for d in result['detections'] if d['type'] == 'PII:ID:NATIONAL']\n",
    "    \n",
    "    if ibans_in_sentence:\n",
    "        extracted_ibans.append(\", \".join(ibans_in_sentence))\n",
    "    else:\n",
    "        extracted_ibans.append(None)\n",
    "\n",
    "\n",
    "processed_df['detected_card'] = extracted_ibans\n",
    "detected_iban_column = processed_df['detected_card']\n",
    "\n",
    "\n",
    "print(processed_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0478c7cc-7888-44e6-a64f-41fa1f0add33",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df.to_csv('tempnational.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "87e463bb-5bff-4d32-91bc-95505e21193d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['sentences', 'PII:ID:NATIONAL', 'detected_card'], dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac262096-6022-46cd-aa11-d2f39b51d747",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d26b1f-c755-46ed-a856-b9abcb2f22b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
